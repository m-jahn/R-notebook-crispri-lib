{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b513802a-dd67-4a81-9a95-fdbd0bfc3670",
   "metadata": {},
   "source": [
    "# Jupyter notebook: sgRNA sequence motifs correlating with repression strength\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter notebook is an attempt to identify sequence motifs or variants in sgRNAs that improve or decrease the efficiency of CRISPRi reression.\n",
    "\n",
    "## Background\n",
    "\n",
    "Two CRISPRi libraries are available for Synechocystis, version 1.0 with only 2 sgRNAs and version 2.0 with on average 5 sgRNAs per gene.\n",
    "Each sgRNA targets the 5'-region of a gene, meaning the transcription start site (TSS) or the region downstream of it.\n",
    "The sgRNA mediates the binding efficiency of the dCas9 enzyme. However, previous and current sequencing results show that the repression strength is by no means uniform.\n",
    "In fact, variation between the efficiency of sgRNAs mediating repression is enormous. Repression efficiency is partly influenced by the distance to the TSS, and partly\n",
    "by unknown sequence features. Such features could be:\n",
    "\n",
    "- folding, secondary structures, such as hairpins\n",
    "- off target binding (competitive binding)\n",
    "- GC content\n",
    "- so far unknown motifs\n",
    "\n",
    "For details regarding the ongoing efforts to determine all gene's fitness contribution, see the [CRISPRi library github repository](https://github.com/m-jahn/R-notebook-crispri-lib) that contains this notebook and further information. The R analysis pipeline that led to the selection of the ncRNAs of interest can be viewed on [m-jahn.github.io](https://m-jahn.github.io/R-notebook-crispri-lib/CRISPRi_V2_data_processing.nb.html).\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- import sgRNA sequences\n",
    "- import sgRNA fitness data and merge with sequences\n",
    "- group sgRNAs in different binding classes (or continuous target variable?)\n",
    "- determine sequence motifs that lead to high repression efficiency and/or correlation\n",
    "- determine sequence motifs that lead to low repression\n",
    "\n",
    "Strategy:\n",
    "\n",
    "- nucleotide sequence is available, needs to be trimmed and aligned\n",
    "- can be one-hot-encoded by position and nucleotide (pos1_A: T/F, pos1_T: T/F, ...)\n",
    "- test and apply different ML models:\n",
    "  - tree-based models including feature importance: Random Forest (RF), gradient boosting (GB)\n",
    "  - NN-based models without feature importance: MLP, CNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31efd641",
   "metadata": {},
   "source": [
    "## Setting up python environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5f9a215",
   "metadata": {},
   "source": [
    "We will use micromamba (or mamba/conda) to generate a reproducible environment for machine learning.\n",
    "In order to do this, create a fresh conda environment:\n",
    "\n",
    "```\n",
    "micromamba create -n machine-learning -c conda-forge\n",
    "source /path/to/micromamba/bin/activate\n",
    "conda activate machine-learnin\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b40389d",
   "metadata": {},
   "source": [
    "Now install the required basic packages:\n",
    "\n",
    "```\n",
    "micromamba install -c conda-forge pandas numpy matplotlib biopython viennarna logomaker ipykernel\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fcd1cf8",
   "metadata": {},
   "source": [
    "And install the required machine learning packages:\n",
    "\n",
    "```\n",
    "micromamba install -c conda-forge scikit-learn tensorflow keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311c3dc-8e3e-4c82-892a-c08a867608ab",
   "metadata": {},
   "source": [
    "## Import of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477871e0-7163-4211-ba18-e8a71a6e327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import Bio.SeqIO\n",
    "import Bio.SeqUtils\n",
    "import subprocess\n",
    "from requests import get\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553477d-dc5c-46c9-8e66-9e06629eeb15",
   "metadata": {},
   "source": [
    "## Data import and re-arrangement\n",
    "\n",
    "The first task is to import the sequence data from a fasta file into a data frame.\n",
    "The data is stored as reference \"genome\" for read mapping, in the [CRISPRi NGS pipeline](https://github.com/m-jahn/CRISPRi-lib-pipe/) on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc1113-e6b9-48ae-ad8d-27e9d41164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_fasta_db = \"https://github.com/m-jahn/CRISPRi-lib-pipe/raw/main/ref/Synechocystis_v2.fasta\"\n",
    "con_fasta_db = get(dir_fasta_db, allow_redirects = True)\n",
    "fasta_db = Bio.SeqIO.parse(StringIO(con_fasta_db.text), \"fasta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "471c649b",
   "metadata": {},
   "source": [
    "- remove the 10 control sgRNAs that are not mapping to anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd75673",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_db = [i for i in fasta_db if not i.id.startswith(\"ctrl\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b03fc70f",
   "metadata": {},
   "source": [
    "- import the reference genome of *Synechocystis* sp. PCC 6803."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "\n",
    "ref_genome = Bio.SeqIO.parse(\n",
    "    \"../data/input/Synechocystis_PCC6803_NC_000911.gbk\", \"genbank\"\n",
    ")\n",
    "ref_genome = list(ref_genome)\n",
    "ref_genome = Seq(\"\").join([i.seq for i in ref_genome])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c6b6653",
   "metadata": {},
   "source": [
    "The next code chunk is important. It loops though the fasta database and, for each guide RNA:\n",
    "\n",
    "- trims adapters away\n",
    "- finds position in genome\n",
    "- determines orientation (0: template strand, 1: non-template strand = rev-com match)\n",
    "- extracts core sequence\n",
    "- extracts 10 nt leading and trailing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c13c0-88d2-446f-ac79-09d227389cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapters to remove\n",
    "fivep_adapter = \"CAGTGATAGAGATACTGGGAGCTA\"\n",
    "threep_adapter = \"GTTTTAGAGCTAGAAATAGCAAGTTAAAAT\"\n",
    "\n",
    "# result data frame\n",
    "fasta_df = pd.DataFrame(\n",
    "    columns=[\"sgRNA\", \"strand\", \"seq\", \"seq_5p\", \"seq_3p\", \"seq_total\", \"len_seq\", \"len_total\"]\n",
    ")\n",
    "\n",
    "# process sgrnas\n",
    "for fasta_record in fasta_db:\n",
    "    threep_pos = fasta_record.seq.find(threep_adapter)\n",
    "    sgrna = fasta_record.seq[len(fivep_adapter) : threep_pos]\n",
    "    if ref_genome.find(sgrna) != -1:\n",
    "        sgrna_strand = 0\n",
    "        ref_genome_local = ref_genome\n",
    "    else:\n",
    "        ref_genome_local = ref_genome.reverse_complement()\n",
    "        sgrna_strand = 1\n",
    "    start_site = ref_genome_local.find(sgrna)\n",
    "    end_site = start_site + len(sgrna)\n",
    "    if start_site == -1 or end_site == -1:\n",
    "        print(fasta_record.id, sgrna, start_site, end_site)\n",
    "        raise Exception\n",
    "    fivep_flank = ref_genome_local[start_site - (30 - len(sgrna)) : start_site]\n",
    "    threep_flank = ref_genome_local[end_site : end_site + 11]\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"sgRNA\": fasta_record.id,\n",
    "            \"strand\": sgrna_strand,\n",
    "            \"seq\": str(sgrna),\n",
    "            \"seq_5p\": str(fivep_flank),\n",
    "            \"seq_3p\": str(threep_flank),\n",
    "            \"seq_total\": str(fivep_flank) + str(sgrna) + str(threep_flank),\n",
    "            \"len_seq\": len(sgrna),\n",
    "            \"len_total\": len(sgrna) + len(fivep_flank) + len(threep_flank),\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    fasta_df = pd.concat([fasta_df.loc[:], new_row]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca26e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1afadd34",
   "metadata": {},
   "source": [
    "Then, we add some extra features determined from sgRNA sequence or position:\n",
    "\n",
    "- binding energy/á¸±inetics using `CRISPRoff` score [1, 2]\n",
    "- GC content\n",
    "- melting temperature\n",
    "- distance to promoter\n",
    "\n",
    "References for `CRISPRoff` score:\n",
    "\n",
    "1. Xiang et al., Enhancing CRISPR-Cas9 gRNA efficiency prediction by data integration and deep learning. Nature Communications, 2021. https://doi.org/10.1038/s41467-021-23576-0.\n",
    "2. Alkan et al., CRISPR-Cas9 off-targeting assessment with nucleic acid duplex energy parameters. Genome Biol, 2018.\n",
    "\n",
    "The `CRISPRoff` score is calculated using the CRISPRoff pipeline downloaded from https://github.com/RTH-tools/crisproff, and the `RNAfold` tool contained in the `viennaRNA` package (installed through conda/mamba previously). Documentation for viennaRNA is [on github](https://github.com/ViennaRNA/ViennaRNA).\n",
    "\n",
    "In order to calculate `CRISPRoff` score, guide RNAs need to be exported as fasta file with a specific format. Every gRNA needs to be exactly 23 nt long, 20 nt spacer + 3 nt PAM site (`NGG`). Then CRISPRoff scores are calculated and results imported again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf338d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/michael/Downloads/crisproff-1.1.2/input.fa', 'w') as fasta_export:\n",
    "    for index, row in fasta_df.iterrows():\n",
    "        pasted_seq = row[\"seq_5p\"] + row[\"seq\"] + row[\"seq_3p\"][0:3]\n",
    "        fasta_line = \">{0}\\n{1}\\n\".format(row[\"sgRNA\"], pasted_seq[-23:])\n",
    "        fasta_export.write(fasta_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.getoutput(\n",
    "    \"cd /home/michael/Downloads/crisproff-1.1.2/;\"\n",
    "    + \"python CRISPRspec_CRISPRoff_pipeline.py \"\n",
    "    + \"--guides input.fa \"\n",
    "    + \"--no_azimuth \"\n",
    "    + \"--guide_params_out 'CRISPRoff_results.tsv'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c361f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crisproff = pd.read_table(\"/home/michael/Downloads/crisproff-1.1.2/CRISPRoff_results.tsv\")\n",
    "df_crisproff = df_crisproff.rename(columns = {\"guideID\": \"sgRNA\", \"CRISPRoff_score\": \"crisproff\"})\n",
    "df_crisproff = df_crisproff[[\"sgRNA\", \"crisproff\"]].set_index(\"sgRNA\")\n",
    "fasta_df = fasta_df.set_index(\"sgRNA\")\n",
    "fasta_df = fasta_df.join(df_crisproff).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add melting temp\n",
    "from Bio.SeqUtils import MeltingTemp\n",
    "fasta_df[\"t_melt\"] = fasta_df[\"seq\"].apply(lambda x: \"%0.2f\" % MeltingTemp.Tm_NN(x))\n",
    "\n",
    "# add GC content\n",
    "from Bio.SeqUtils import gc_fraction\n",
    "fasta_df[\"gc\"] = fasta_df[\"seq\"].apply(lambda x: gc_fraction(x))\n",
    "\n",
    "# add sgRNA distance to promoter\n",
    "fasta_df[\"dist\"] = fasta_df[\"sgRNA\"].apply(lambda x: int(x.split(\"|\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04933036-44e9-4cdd-beeb-8b767b5129f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f178d-2279-475d-b414-c03a6ee7c351",
   "metadata": {},
   "source": [
    "The second task is to import the main data table with fitness, correlation and repression efficiency for every sgRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e8ac9-36fb-4a9b-bb92-b63cdac137f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/output/fitness_sgRNA.csv\")\n",
    "data.iloc[:, 0:14].head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7bb6d-e7b9-419b-aa2b-6e1fd69f4afb",
   "metadata": {},
   "source": [
    "The data frame needs to be reduced/summarized to the desired shape. We don't need fitness and log2FC for all conditions and time points.\n",
    "All we need instead is 1 sgRNA per row, with:\n",
    "\n",
    "- mean absolute fitness effect over all conditions\n",
    "- repression efficiency (relative score between 0 and 1)\n",
    "- correlation (also relative score between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098704bb-2ea7-45df-a083-b2b2450836c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = data[[\"sgRNA\", \"sgRNA_target\", \"sgRNA_position\", \"sgRNA_correlation\", \"sgRNA_efficiency\", \"fitness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b768d-11d6-4b08-9aae-dabf25a6b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = data_reduced.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d6d67-0ed7-4cc2-aad7-df2602a56b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all sgRNAs to mean abs fitness etc\n",
    "data_reduced[\"fitness\"] = abs(data_reduced[\"fitness\"])\n",
    "data_reduced = data_reduced.groupby([\"sgRNA_target\", \"sgRNA\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8132d-51b9-48e4-9f3a-2d7f2039d0f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparation for ML training and tuning\n",
    "\n",
    "1. Remove non-responsive genes (no sgRNA has an effect on fitness)\n",
    "2. Choose and optionally scale/bin target variable\n",
    "3. One-hot encode sgRNA sequences\n",
    "4. Split data into training and test data set\n",
    "\n",
    "### Task 1: Remove non-responsive genes\n",
    "\n",
    "Plot distribution of sgRNA fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b722a3-4af5-4743-a769-2afb4162c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(data_reduced[\"fitness\"]), bins = 30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fd994-b002-49f3-a1ba-6f4de0a178a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs(data_reduced[\"fitness\"]), bins = 30, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d46ad-1e11-4f83-a78c-ffe68c764937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all genes out where no sgRNA exceeds fitness effect > 1 in any condition\n",
    "list_max_fitness = data_reduced.groupby(\"sgRNA_target\")[\"fitness\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d0309-2deb-44ef-bf4d-8c255f1fc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_max_fitness = list_max_fitness[abs(list_max_fitness) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7acff-e71b-4e09-8934-55e021661589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = data_reduced.reset_index().merge(list_max_fitness, on = \"sgRNA_target\")\n",
    "data_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807579f-40b0-4a65-adc6-2b67457c9d8e",
   "metadata": {},
   "source": [
    "### Task 2: Scale and bin target variable\n",
    "\n",
    "Target variable is sgRNA efficiency, a relative score between 0 (no effect on fitness) and 1 (maximum effect on fitness).\n",
    "Scaling can therefore be omitted.\n",
    "\n",
    "- if target variable is taken as is, the ML problem becomes a regression problem\n",
    "- if target variable is binned into e.g. 5 bins of width 0.2, the problem becomes a classification problem\n",
    "- choose simplest possible classification in high and low efficcieny sgRNAs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec0583-a300-4e7a-b542-70f4e8d1b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bins = [\"0-low\", \"1-high\"]\n",
    "data_reduced[\"efficiency_binned\"] = pd.cut(data_reduced[\"sgRNA_efficiency\"], len(list_bins), labels = list_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1eb1b-7a81-4397-ac2d-a1c22b8ef4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bin_sum = data_reduced.groupby(\"efficiency_binned\").size()\n",
    "plt.bar(x = data_bin_sum.index, height=data_bin_sum);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11729882-ea4a-4702-bbbd-aedf8f872f15",
   "metadata": {},
   "source": [
    "### Task 3: Prepare feature matrix\n",
    "\n",
    "- For sequence-based features, we use the entire sgRNA region: 5'-flank (10-15) | spacer (17-22) | PAM-site (3) | 3'-flank (10)\n",
    "- alternative is to pad shorter guideRNA spcacers with 0-5 N nucleotides (currently not done)\n",
    "- One hot encoding of sgRNA sequences\n",
    "- rescaling of the other non-sequence based features to range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf4ead-fcb8-4180-9c43-026af9800a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter fasta df to only sgRNAs that show an effect (common set with data)\n",
    "fasta_df_reduced = pd.merge(fasta_df, data_reduced, how = 'inner', on = 'sgRNA')\n",
    "print(min(fasta_df_reduced.len_seq), max(fasta_df_reduced.len_seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b36a9d9-77e1-446d-846d-1c57f4f86b6a",
   "metadata": {},
   "source": [
    "**Shape of the sequence data as input for scikit learn**\n",
    "\n",
    "- data is always a 2D array, shape (n_samples, n_features)\n",
    "- features should be **one hot encoded**: all cateogrical variables are spread to a list/array with binary encoding (0 OR 1) for yes and no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50b37c-1216-4343-a8e4-7cb8345f9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(s):\n",
    "    z = []\n",
    "    for nt in str(s):\n",
    "        if not nt in \"AaCcGgTt\":\n",
    "            print(\"Non-ATGC character: \" + nt, \"sgRNA: \" + s)\n",
    "            raise Exception\n",
    "        if nt in \"Aa\": z.append(1)\n",
    "        else: z.append(0)\n",
    "        if nt in \"Cc\": z.append(1)\n",
    "        else: z.append(0)\n",
    "        if nt in \"Gg\": z.append(1)\n",
    "        else: z.append(0)\n",
    "        if nt in \"Tt\": z.append(1)\n",
    "        else: z.append(0)\n",
    "    return(z)\n",
    "\n",
    "fasta_df_reduced[\"seq_onehot\"] = fasta_df_reduced.seq_total.apply(onehot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48ddeb62",
   "metadata": {},
   "source": [
    "- other non-sequence features are rescaled to a mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_other = fasta_df_reduced[[\"len_seq\", \"crisproff\", \"t_melt\", \"gc\", \"dist\"]].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "sklearn_scaler = preprocessing.StandardScaler().fit(array_other)\n",
    "array_other = sklearn_scaler.transform(array_other)\n",
    "\n",
    "# check mean and standard deviation\n",
    "print(array_other.mean(axis=0))\n",
    "print(array_other.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d0dde-9928-49a7-9c69-614d64a5e4b6",
   "metadata": {},
   "source": [
    "- finally turn pd.Series/DF into an array with dimensions n_observations x n_features\n",
    "- also turn target variable into an array of length n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b82039-4424-44c9-beda-fda5fb35a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_fasta = np.array(fasta_df_reduced.seq_onehot.to_list())\n",
    "array_target = fasta_df_reduced.efficiency_binned.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d6bc80",
   "metadata": {},
   "source": [
    "- combine array with sequence features and array with additional features into one-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a889aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_fasta = np.concatenate((array_fasta, array_other), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6123e80-195c-455d-91bd-7e3b2a74e8d4",
   "metadata": {},
   "source": [
    "- split into training and validation set (75%/25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3f103-73de-450e-af4d-4b9b142e3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# split data automatically with this short hand\n",
    "x_train, x_test, y_train, y_test = train_test_split(array_fasta, array_target, test_size = 0.25, random_state = 42)\n",
    "# obtain also indices to select rows of the original data\n",
    "for i_train, i_test in ShuffleSplit(n_splits = 1, test_size = 0.25, random_state = 42).split(array_fasta):\n",
    "    print('Number of observations for training data: ' + str(len(i_train)))\n",
    "    print('Number of observations for test data: ' + str(len(i_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c81890-c8b8-4acd-ae6b-6362444b3631",
   "metadata": {},
   "source": [
    "## Logos to visualize sequence motifs\n",
    "\n",
    "- logos are visualizations for nucleotide probability or prevalence of a set of aligned sequences\n",
    "- can use the python package `logomaker` installed with `pip install logomaker`\n",
    "- separate logos for low, (medium), high efficiency sgRNAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f0b68-ec8d-4ef7-a3b0-e78a83a2b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logomaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279dd07-e4d9-4851-ab1a-625574b4f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colors for the four bases\n",
    "colors = {'A': '#E7298A', 'C': '#66A61E', 'G': '#E6AB02', 'T': '#999999'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cae912-b08d-4ea5-be85-6a6faf7c42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a pd.Series with sequences as input and plot a logo\n",
    "def plot_logo(data, to_type = 'weight', ignore = 'N', title = '', ylim = [-0.85, 0.6]):\n",
    "    data_str = [str(i) for i in data]\n",
    "    data_logo = logomaker.alignment_to_matrix(\n",
    "        data_str, to_type = to_type,\n",
    "        characters_to_ignore = ignore)\n",
    "    data_logo.index = data_logo.index + 1\n",
    "    title = '{0} (n = {1})'.format(title, len(data_str))\n",
    "    logo = logomaker.Logo(\n",
    "        data_logo,\n",
    "        fade_below = 0.33,\n",
    "        color_scheme = colors\n",
    "    )\n",
    "    logo.ax.set_ylim(ylim)\n",
    "    logo.ax.text(0, ylim[1]+0.05, title,fontsize = 14)\n",
    "    logo.ax.set_xticks(np.arange(2,len(data_str[0])+1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38cf2e-d54d-4794-b32b-f30c738e2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logo for ALL sgRNAs combined\n",
    "plot_logo(fasta_df_reduced.seq_total, title = 'All sgRNAs', ylim = [-1.6, 0.9])\n",
    "plt.savefig('../figures/logo_all.pdf')\n",
    "\n",
    "# Logo for LOW EFFICIENCY sgRNAs\n",
    "for i in list_bins:\n",
    "    plot_logo(\n",
    "        data = fasta_df_reduced.query('efficiency_binned == \"' + i + '\"').seq_total,\n",
    "        title = i,\n",
    "        ylim = [-1.6, 0.9])\n",
    "    plt.savefig('../figures/logo_' + i + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a32392-9449-4517-b096-c0e265475333",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- clear relationship of sgRNA sequence and repression efficiency\n",
    "- high efficiency sgRNAs enriched for 'G' and depleted for 'A' or 'T' in position 4-14 proximal to PAM site\n",
    "- low efficiency sgRNAs are not enriched for a specific sequence pattern (no neg. selection criterion)\n",
    "- med efficiency sgRNAs are in fact a mixture of the two above patterns (slight enrichment of 'G' stretch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4210d6d-0d07-4357-a259-c5f7c522200f",
   "metadata": {},
   "source": [
    "## Model 1: Support vector machine\n",
    "\n",
    "Some useful advice from the scikit-learn page\n",
    "\n",
    "- Avoiding data copy: Data passed to certain methods will be copied before calling the underlying C implementation. Check if numpy array is C-contiguous by inspecting its flags attribute\n",
    "- Kernel cache size: has a strong impact on run times for larger problems. If you have enough RAM available, increase cache_size to a higher value\n",
    "- Setting C: C is 1 by default and itâs a reasonable default choice. If you have a lot of noisy observations you should decrease it\n",
    "- Larger C values will take more time to train, sometimes up to 10 times longer\n",
    "- SVM algorithms are not scale invariant, so it is highly recommended to scale your data (scale all features to [0,1] or [-1,+1]. This was done in our example.\n",
    "- Kernel functions: can be  one of linear (most simple), polynomial (`degree`, `coef0`), radial basis function (RBF, `gamma` > 0) or sigmoid (`coef0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a3e30-939f-4763-b8f4-8d528d4ca579",
   "metadata": {},
   "source": [
    "Important parameters for fitting:\n",
    "\n",
    "- `kernel`: the main parameter that determines the type of hyperplane for dividing n-D feature vectors\n",
    "- `n_jobs = -1`: uses all cores instead of 1 (the default)\n",
    "- `cv`: number of k-fold cross validations, default 5 can be left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c5976-5159-48fb-a309-af4bfffc0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292b975-532e-4213-90d1-e62835307e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "param_grid = {\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'C': [0.5, 0.75, 1, 1.5, 2],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.5, 1, 5]}\n",
    "SVM = svm.SVC(random_state = 42)\n",
    "SVM_tune = GridSearchCV(SVM, param_grid, n_jobs = -1)\n",
    "SVM_tune.fit(x_train[: 2000], y_train[: 2000])\n",
    "print(SVM_tune.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45624719-2ab1-4c63-a121-5b4004b8733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting & prediction\n",
    "SVM = svm.SVC(**SVM_tune.best_params_, probability = True, class_weight = \"balanced\")\n",
    "SVM_fit = SVM.fit(x_train, y_train)\n",
    "SVM_pred = SVM_fit.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa7776-0d39-42de-8a3b-fdd5716b69fa",
   "metadata": {},
   "source": [
    "Evalauation of model performance:\n",
    "\n",
    "- `precision` = fraction of relevant instances among the retrieved instances (TP/(TP+FP))\n",
    "- `recall` = fraction of relevant instances that were retrieved (also called sensitivity, TP/(TP+FN))\n",
    "- `f1-score` = harmonic mean of the precision and recall, best value at 1 and worst at 0\n",
    "- `specificity` - fraction of non-relevant instances that were retrieved (TN/(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bbdff-07f4-457d-8796-ebb67657b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM train accuracy: %0.3f\" % SVM_tune.score(x_test, y_test))\n",
    "print(classification_report(y_test, SVM_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f4451-ace7-4bc8-8914-ead46a0726a9",
   "metadata": {},
   "source": [
    "- test predictive power of the model\n",
    "- predict classes of validation data set and make confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492a285-3ef7-4d24-8a8c-4375176cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis: predictions; y-axis: ground truth\n",
    "SVM_cm = confusion_matrix(y_test, SVM_pred, labels = SVM_fit.classes_)\n",
    "SVM_cmplot = ConfusionMatrixDisplay(SVM_cm, display_labels = SVM_fit.classes_)\n",
    "SVM_cmplot.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114a15b-8eb5-4e25-aa3e-daba7fced58b",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n",
    "\n",
    "- Both model types decision tree based, RF with global tree refinement, GBM with local iterative refinement\n",
    "- Fully tractable models: not only prediction of classes, but also feature importance\n",
    "- Very important aspect of our question: Which positions/nucleotides of the sgRNA are particularly important?\n",
    "- This question is almost more important than prediciting quality of sgRNAs, as it can be used to derive generalized rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7649f5-fb17-42e3-b287-d094165dde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65da79-8645-4bff-8434-e23f7b066613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 200, 300, 400],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [5, 10, 25, 50, 100]}\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 42)\n",
    "RF_tune = GridSearchCV(RF, param_grid, n_jobs = -1)\n",
    "RF_tune.fit(x_train[: 2000], y_train[ : 2000])\n",
    "print(RF_tune.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37934f3f-6238-47ee-bd0d-58f8f15f97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting & prediction\n",
    "RF = RandomForestClassifier(**RF_tune.best_params_, class_weight = \"balanced\")\n",
    "RF_fit = RF.fit(x_train, y_train)\n",
    "RF_pred = RF_fit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d508d47-3664-41b8-a20b-091f2927dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF train accuracy: %0.3f\" % RF_tune.score(x_test, y_test))\n",
    "print(classification_report(y_test, RF_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6be315-6234-42d6-b581-51f6631c4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cm = confusion_matrix(y_test, RF_pred, labels = SVM_fit.classes_)\n",
    "RF_cmplot = ConfusionMatrixDisplay(RF_cm, display_labels = SVM_fit.classes_)\n",
    "RF_cmplot.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b490cc2-725e-43e0-a466-6a8dda817246",
   "metadata": {},
   "source": [
    "- finally visualize **feature importance** for the RF model\n",
    "- SVMs and NNs do not have feature importance readily available, they are more black box models\n",
    "- since the model does not (yet) performs well, the feature importance has not much meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2623978-7400-4e1c-90cb-b74f79df43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sgRNAs = int(fasta_df_reduced[\"len_total\"].aggregate(\"mean\"))\n",
    "\n",
    "RF_feat_imp = pd.DataFrame()\n",
    "RF_feat_imp['feat_imp'] = pd.Series(RF_fit.feature_importances_)[0:(4*len_sgRNAs)]\n",
    "RF_feat_imp['position'] = np.repeat(np.arange(len_sgRNAs), 4)\n",
    "RF_feat_imp['base'] = ['A','C','G','T']*len_sgRNAs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), RF_feat_imp.query('base == \"A\"').feat_imp, label = 'A', color = colors['A'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), RF_feat_imp.query('base == \"C\"').feat_imp, label = 'C', color = colors['C'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), RF_feat_imp.query('base == \"G\"').feat_imp, label = 'G', color = colors['G'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), RF_feat_imp.query('base == \"T\"').feat_imp, label = 'T', color = colors['T'])\n",
    "ax.set_ylabel('feature importance')\n",
    "ax.set_xlabel('position [bp]')\n",
    "ax.axis(ymin = 0.003, ymax = 0.012)\n",
    "ax.legend()\n",
    "ax.grid(color='lightgrey')\n",
    "plt.savefig('../figures/plot_rf_feat_importance.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ece33-2905-4155-9dfa-5e41cea1d7b4",
   "metadata": {},
   "source": [
    "## Model 3: Gradient boosting machine (GBM)\n",
    "\n",
    "- based on decision trees, similar to RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272821ab-6279-4d6d-a9b8-943e83e36836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f733906-7566-47fe-a222-f7cf9249b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "param_grid = {\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'max_depth': [1]}\n",
    "\n",
    "GBM = GradientBoostingClassifier(random_state = 42)\n",
    "GBM_tune = GridSearchCV(GBM, param_grid, n_jobs = -1)\n",
    "GBM_tune.fit(x_train[: 2000], y_train[ : 2000])\n",
    "print(GBM_tune.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9588a-06a4-474f-aa2f-5ee6c603247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting & prediction\n",
    "GBM = GradientBoostingClassifier(**GBM_tune.best_params_)\n",
    "GBM_fit = GBM.fit(x_train, y_train)\n",
    "GBM_pred = GBM_fit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5041172-413a-4b96-9e97-bb6ca47bedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GBM train accuracy: %0.3f\" % GBM_tune.score(x_test, y_test))\n",
    "print(classification_report(y_test, GBM_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1c9ac-a952-4b0e-8e0e-4c11d9c09653",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM_cm = confusion_matrix(y_test, GBM_pred, labels = SVM_fit.classes_)\n",
    "GBM_cmplot = ConfusionMatrixDisplay(GBM_cm, display_labels = SVM_fit.classes_)\n",
    "GBM_cmplot.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae97b8-3b55-4f9a-a804-9a3b3d153037",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM_feat_imp = pd.DataFrame()\n",
    "GBM_feat_imp['feat_imp'] = pd.Series(GBM_fit.feature_importances_)[0:(4*len_sgRNAs)]\n",
    "GBM_feat_imp['position'] = np.repeat(np.arange(len_sgRNAs), 4)\n",
    "GBM_feat_imp['base'] = ['A','C','G','T']*len_sgRNAs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), GBM_feat_imp.query('base == \"A\"').feat_imp, label = 'A', color = colors['A'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), GBM_feat_imp.query('base == \"C\"').feat_imp, label = 'C', color = colors['C'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), GBM_feat_imp.query('base == \"G\"').feat_imp, label = 'G', color = colors['G'])\n",
    "ax.plot(np.arange(1,len_sgRNAs+1), GBM_feat_imp.query('base == \"T\"').feat_imp, label = 'T', color = colors['T'])\n",
    "ax.set_ylabel('feature importance')\n",
    "ax.set_xlabel('position [bp]')\n",
    "ax.legend()\n",
    "ax.grid(color='lightgrey')\n",
    "plt.savefig('../figures/plot_gbm_feat_importance.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94c25d-4707-4f2f-b67f-d4ca7166dc4f",
   "metadata": {},
   "source": [
    "Conclusions from all models so far:\n",
    "\n",
    "- tree based methods performed less good than SVM\n",
    "- tree-based methods had high recall/senstivity for low-eff sgRNAs, but low recall for high-eff sgRNAs\n",
    "- feature importance from RF and GBM clearly shows that \n",
    "- further steps: try a CNN, or try to predict interesting seq features from SVM by probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6965be8c-8c45-4273-abdd-152c52ef506d",
   "metadata": {},
   "source": [
    "## Model 4: Multi-layer perceptron\n",
    "\n",
    "- for this purpose we try tensorflow + keras\n",
    "- some background: MLP or other neural networks consist of one or several layers of 'neurons' or nodes\n",
    "- input data can be multidimensional, in our case it's simply a one-hot-encoded array of i observations x j features\n",
    "- input features are passed to first layer of NN, transformed by a function, and passed to the next layer\n",
    "- nodes are highly interconnected so that traits from multiple features get 'recognized' and passed to next layer\n",
    "- each node generates only one output value $y$ from multiple feature inputs $x$: $y = \\sigma \\times (\\sum(w_i \\times x_i) + b)$\n",
    "- with $x$ being a feature value, $w$ a weight factor, $b$ a learnable bias, and $\\sigma$ an activation/cost function)\n",
    "- training a neural network usually means to vary (optimize) the weight parameter $w$ for each node such that the cost (difference between expected output and obtained output) is minimized\n",
    "- one difference to the other ML models is that target variable (labels) need to be one hot encoded too\n",
    "- y therefore needs to be an array with i observations times j number of different labels\n",
    "- the final layer of the model must have as many nodes as there are labels in the target variable (here: 2, low and high eff.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ee211-6f22-4966-b83e-9eb0defb079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3ad80-d6ca-4887-9afb-aab63f69a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess labels: convert to numerical, and also one-hot-encode\n",
    "# similar to input features (binary output)\n",
    "y_train_ohe = preprocessing.LabelEncoder().fit_transform(y_train)\n",
    "y_test_ohe = preprocessing.LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "y_train_ohe = utils.np_utils.to_categorical(y_train_ohe, num_classes = len(list_bins))\n",
    "y_test_ohe = utils.np_utils.to_categorical(y_test_ohe, num_classes = len(list_bins))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train_ohe.shape)\n",
    "print(y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a194-060c-4b83-927c-4c9ce829a56b",
   "metadata": {},
   "source": [
    "- build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce188101-9063-46bf-9b03-038d5a34bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = Sequential(name='MLP')\n",
    "\n",
    "# Layer 1\n",
    "MLP.add(layers.core.Dense(64, activation='relu', input_dim=x_train.shape[1]))\n",
    "\n",
    "# Layer 2\n",
    "MLP.add(layers.core.Dense(128, activation='relu'))\n",
    "\n",
    "# Layer 3\n",
    "MLP.add(layers.core.Dense(len(list_bins), activation='softmax'))\n",
    "MLP.compile(optimizers.sgd_experimental.SGD(learning_rate = 0.01, momentum = 0.9, nesterov = True),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe5232-994f-4fd3-bffc-3134595dbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MLP.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb65d7-c925-4947-b1f8-621df83f5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_fit = MLP.fit(x_train, y_train_ohe, epochs = 200, batch_size = 32, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dea24-aa82-4ef3-a20c-b801af476391",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.evaluate(x_test, y_test_ohe)\n",
    "MLP_pred = MLP.predict(x_test)\n",
    "MLP_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e271886-e07d-4925-b650-02239b6a3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert OHE array back to factor with labels\n",
    "y_test_class = np.argmax(y_test_ohe, axis=1)\n",
    "y_pred_class = np.argmax(MLP_pred, axis=1)\n",
    "\n",
    "MLP_cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "MLP_cmplot = ConfusionMatrixDisplay(MLP_cm)\n",
    "MLP_cmplot.plot();\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfd7e0-d16f-4774-ab01-f2f66d48fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy and loss over model training\n",
    "df_MLP = pd.DataFrame.from_dict(MLP_fit.history)\n",
    "df_MLP['accuracy'].plot(legend = True);\n",
    "df_MLP['loss'].plot(legend = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dba23d-060d-431b-89b2-68a5821b6c7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Conclusions:\n",
    "\n",
    "- best performing models were SVM and MLP\n",
    "- overall accuracy 60% which is not great\n",
    "- precision and recall are highest (70%) for low efficency sgRNAs\n",
    "- precision and recall around 50% for high efficiency sgRNAs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac3f2abb-1835-45c0-9887-2884655d45e3",
   "metadata": {},
   "source": [
    "## Sequence motifs of high efficiency sgRNAs\n",
    "\n",
    "- summarize predictions of all models in order to increase high confidence sgRNAs\n",
    "- inspect sgRNAs with top predicted performance closer\n",
    "- try to identify sequence patterns that might be enriched using clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d52d6-f862-496f-8443-8833689641f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve correctly identified sgRNAs from all 4 models (score between 0 and 4)\n",
    "list_high_eff = (y_test_class == y_pred_class).astype(int) +\\\n",
    "    (y_test == SVM_pred).astype(int) +\\\n",
    "    (y_test == RF_pred).astype(int) +\\\n",
    "    (y_test == GBM_pred).astype(int)\n",
    "\n",
    "print('High effiency sgRNAs in test data identified by at least one model: ' + str(sum(list_high_eff[y_test_class == 1] >= 1)))\n",
    "print('High effiency sgRNAs in test data not identified by any model: ' + str(sum(list_high_eff[y_test_class == 1] < 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bd57a-615d-4426-b60a-5ba68a8aab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = pd.DataFrame(list_high_eff[y_test_class == 1], columns=['hits']).value_counts()\n",
    "df_ensemble = df_ensemble.sort_index()\n",
    "print(df_ensemble)\n",
    "\n",
    "plt.bar(x = range(df_ensemble.shape[0]), height=df_ensemble)\n",
    "plt.savefig('../figures/plot_barchart_ML_recall.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c3fd3-4c1e-48f3-a30d-e13fd0198355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logical with length of test data set indexing subset of\n",
    "# true positive AND model positive sgRNAs\n",
    "i_modelpos = i_test[np.logical_and(y_test_class == 1, list_high_eff >= 1)]\n",
    "i_modelneg = i_test[np.logical_and(y_test_class == 1, list_high_eff <  1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dff004-223c-4755-b091-9ae47572c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logo for high efficiency sgRNAs identified by ML models\n",
    "plot_logo(\n",
    "    fasta_df_reduced.iloc[i_modelpos].seq_total,\n",
    "    title = 'high eff. sgRNAs retrieved by >= 1 model',\n",
    "    ylim = [-1.6, 0.9])\n",
    "\n",
    "plt.savefig('../figures/logo_ML_1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822c33c-0756-4cf4-8f00-9b139ecb5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logo for high efficiency sgRNAs NOT identified by ML models\n",
    "plot_logo(\n",
    "    fasta_df_reduced.iloc[i_modelneg].seq_total,\n",
    "    title = 'high eff. sgRNAs retrieved by 0 models',\n",
    "    ylim = [-1.6, 0.9])\n",
    "\n",
    "plt.savefig('../figures/logo_ML_2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df054c-3d6f-47c5-9a05-11b23a07c7e9",
   "metadata": {},
   "source": [
    "## Pie charts to summarize sgRNAs used for modeling\n",
    "\n",
    "Simple pie charts showing fractions of:\n",
    "\n",
    "- used sgRNAs (genes) vs all sgRNAs (genes) (2 fractions)\n",
    "- low efficiency vs high efficiency sgRNAs (2 fractions)\n",
    "- train and test sets, of the latter the model identified and non-identified high eff subset (3 fractions)data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a535fd-49bf-4a34-931c-67e67904bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure and axes\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "# Pie 1: total vs used sgRNAs\n",
    "n_total_sgRNA = len(data.sgRNA.unique())\n",
    "n_used_sgRNA = len(data_reduced.sgRNA.unique())\n",
    "\n",
    "axs[0, 0].pie(\n",
    "    [n_total_sgRNA-n_used_sgRNA, n_used_sgRNA],\n",
    "    labels=['other sgRNAs\\n{}'.format(n_total_sgRNA-n_used_sgRNA), 'used sgRNAs\\n{}'.format(n_used_sgRNA)],\n",
    "    shadow=True, explode=[0.05,0.05])\n",
    "\n",
    "# Pie 2: low vs high eff sgRNAs\n",
    "n_low_high_sgRNA = data_bin_sum.to_list()\n",
    "axs[0, 1].pie(\n",
    "    n_low_high_sgRNA,\n",
    "    labels=['low eff. sgRNAs\\n{}'.format(n_low_high_sgRNA[0]), 'high eff. sgRNAs\\n{}'.format(n_low_high_sgRNA[1])],\n",
    "    shadow=True, explode=[0.05,0.05])\n",
    "\n",
    "# Pie 3: train and test set, test subdivided in ML+ and ML-\n",
    "n_train_test = [fasta_df_reduced.iloc[i_train].query('efficiency_binned==\"1-high\"').shape[0],\n",
    "    len(i_modelneg), len(i_modelpos)]\n",
    "axs[1, 0].pie(\n",
    "    n_train_test,\n",
    "    labels=['training\\n{}'.format(n_train_test[0]),\n",
    "        'test, not ident\\n{}'.format(n_train_test[1]),\n",
    "        'test, identified.\\n{}'.format(n_train_test[2])],\n",
    "    shadow=True, explode=[0.05,0.05,0.05])\n",
    "\n",
    "# Hide last position\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "plt.savefig('../figures/piechart_ML.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60ba99cc",
   "metadata": {},
   "source": [
    "## Non-sequence feature importance\n",
    "\n",
    "- apart from the 210 nucleotides that each are a feature, we have 5 more non-binary features\n",
    "- these are: length of sgRNA, `crisproff` score, melting temp, GC-content, and distance to promoter\n",
    "- their feature importance is evaluated in comparison to sequence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Other_feat_imp = pd.DataFrame()\n",
    "Other_feat_imp[\"parameter\"] = fasta_df_reduced.columns[[6, 8, 9, 10, 11]]\n",
    "Other_feat_imp[\"RF_imp\"] = RF_fit.feature_importances_[(4 * len_sgRNAs) :]\n",
    "Other_feat_imp[\"GBM_imp\"] = GBM_fit.feature_importances_[(4 * len_sgRNAs) :]\n",
    "\n",
    "for param in [\"RF_imp\", \"GBM_imp\"]:\n",
    "    labels = Other_feat_imp.parameter\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x - 0.35 / 2, Other_feat_imp.get(param), 0.35, label=param)\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'../figures/plot_{param}_other.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ec4eed5-284c-4315-9242-c0e27270abff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GC content as efficiency predictor\n",
    "\n",
    "- it is obvious that some of the \"quality\" (or efficiency) can be explained by GC content\n",
    "- as a matter of fact, all high eff sgRNAs seem to have higher GC content from logo pattern\n",
    "\n",
    "GC content needs to be investigated as a confounding variable, because that alone is a strong predictor compared to sequence motifs.\n",
    "However if ML models pick GC content up as predictor, it's not wrong, only less useful.\n",
    "\n",
    "- task 1: compare GC content for high and low efficiency group\n",
    "- task 2: quantify GC skew and compare between groups\n",
    "- task 3: if necessary, balance the groups such that average GC content is even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16ee4c-e0ff-4747-b797-d3b5cb7e0d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fasta_df_reduced['gc_skew'] = [Bio.SeqUtils.GC_skew(i, window = 100)[0] for i in fasta_df_reduced.seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4825d-d29a-488c-8aed-983dce572c5d",
   "metadata": {},
   "source": [
    "Task 1: Average GC content per group:\n",
    "\n",
    "- high vs low efficiency sgRNAs, total dataset (6400 sgRNAs)\n",
    "- high efficiency sgRNAs, model-retrieved versus non-retrieved (400 vs 200 sgRNAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685064f8-63d9-43ba-aa94-3aa29b399628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for eff in list_bins:\n",
    "    eff_subset = fasta_df_reduced[fasta_df_reduced.efficiency_binned == eff]['gc']\n",
    "    plt.hist(eff_subset, bins = 19, alpha = 0.5, label = eff, density = True)\n",
    "    print('Mean GC content for {0} is: {1}'.format(eff, str(round(eff_subset.mean(), 2))))\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265df651-eb85-4c77-95db-d8a92738876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "dict_groups = {'true pos': i_modelpos, 'false neg': i_modelneg}\n",
    "for group in dict_groups:\n",
    "    eff_subset = fasta_df_reduced.iloc[dict_groups.get(group)].gc\n",
    "    plt.hist(eff_subset, bins = 19, alpha = 0.5, label = group, density = True)\n",
    "    print('Mean GC content for {0} is: {1}'.format(group, str(round(eff_subset.mean(), 2))))\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a7d84-b053-4e01-9fe5-383391a3cb9d",
   "metadata": {},
   "source": [
    "Task 2: GC skew per group\n",
    "\n",
    "- high vs low efficiency sgRNAs, total dataset (6400 sgRNAs)\n",
    "- high efficiency sgRNAs, model-retrieved versus non-retrieved (400 vs 200 sgRNAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d278e-dac9-4be5-a49f-7565ef693db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for eff in list_bins:\n",
    "    eff_subset = fasta_df_reduced[fasta_df_reduced.efficiency_binned == eff]['gc_skew']\n",
    "    plt.hist(eff_subset, bins = 19, alpha = 0.5, label = eff, density = True)\n",
    "    print('Mean GC skew for {0} is: {1}'.format(eff, str(round(eff_subset.mean(), 2))))\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd60b2-188b-4c82-b37f-361446c39a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for group in dict_groups:\n",
    "    eff_subset = fasta_df_reduced.iloc[dict_groups.get(group)].gc_skew\n",
    "    plt.hist(eff_subset, bins = 19, alpha = 0.5, label = group, density = True)\n",
    "    print('Mean GC skew for {0} is: {1}'.format(group, str(round(eff_subset.mean(), 2))))\n",
    "plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d4b57-383a-42cc-9327-8e15bc74125d",
   "metadata": {},
   "source": [
    "Test if GC content is significantly different for low and high-efficiency sgRNAs. We use a non-parametric test because the distribution is not normal/gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dac93-bfe6-47c6-aa87-bc961069ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "for metric in ['gc', 'gc_skew']:    \n",
    "    wilcox = stats.ranksums(\n",
    "        fasta_df_reduced[fasta_df_reduced.efficiency_binned == '0-low'][metric],\n",
    "        fasta_df_reduced[fasta_df_reduced.efficiency_binned == '1-high'][metric],\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    print(wilcox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5f510-cb02-4de0-8c43-c9b06e37bc81",
   "metadata": {},
   "source": [
    "The same test for the high efficiency sgRNAs, model-retrieved versus non-retrieved (400 vs 200 sgRNAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4a6bb-1242-4bff-b907-00943be92d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['gc', 'gc_skew']:    \n",
    "    wilcox = stats.ranksums(\n",
    "        fasta_df_reduced.iloc[dict_groups.get('true pos')][metric],\n",
    "        fasta_df_reduced.iloc[dict_groups.get('false neg')][metric],\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    print(wilcox)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f203f70-ae67-416c-862a-926cddfcfb8b",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "- average GC content is 2% higher in high eff. sgRNAs, not substantially\n",
    "- difference in GC content is small but signigicant (p ~ 10^-20)\n",
    "- average GC skew (G-C)/(G+C) is -0.02 in low eff sgRNAs, and 0.09 in high eff sgRNAs\n",
    "- that corresponds to around +1G for every 5C in high eff sgRNAs\n",
    "- also this difference was highly significant\n",
    "- average GC content between model-retrieved and non-retrieved high eff sgRNAs is more different (5.5%)\n",
    "- same for GC skew, difference twice as high as high-vs-low effiency comparison\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- models succeeded to predict high eff sgRNAs _if they have high GC content_ but otherwise not\n",
    "- we can not say that GC content is the best/only predictor, as the original data set is balanced\n",
    "- high eff sgRNAs _with low GC content_ apparently evade prediction due to lack of other good features\n",
    "- the entire data set might be too small to discover all features that determine sgRNA efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "a14a95c5e06e7b6ee1bfec6df417028bf94f9e31a62048ef944d37cad2f673f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
