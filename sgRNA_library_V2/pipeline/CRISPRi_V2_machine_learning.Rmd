---
title: "CRISPRi library V2, machine learning pipeline"
output:
  html_notebook: 
    theme: cosmo
    toc: yes
    number_sections: true
---

----------

# Description

## Problem definition

This R notebook applies a machine learning approach to the gene fitness data obtained from CRISPRi library repression and competition experiments. There are two principal strategies how the available data could be applied in a machine learning work flow:

1. Functional classification of unknown genes based on fitness pattern of known/annotated genes. *Response variable*: functional category, such as KEGG pathway, *Predictor(s)*: fitness in different conditions, other external features.
2. Prediction of fitness contribution of unknown genes based on functional annotation of known genes. *Response variable*: fitness for one gene in one condition, *Predictor(s)*: functional categories, other external features.

The first objective, classification or at least association of genes with certain functions, seems the more rewarding task as it allows to gain new biological insights into unknown genes. Generally, the number of features per gene (fitness measured in different conditions) is relatively low, around 10 conditions. It might be necessary to include more features based on information deposited in databases. Such information could include for example:

1. single sgRNA fitness over time, instead of summarized fitness score
2. pathway or functional annotation, e.g. in the form of GO terms
3. sequence motifs, functional prediction (PFAM, DeepEC EC number)
4. interaction with other genes (STRING DB)

The input data has several important characteristics:

- target variable for the classifier are KEGG pathways. There can be multiple classes (--> **labels**) per gene
- target variable is very diverse (more than 100 labels). It might be good to use a simpler classification target
- features contain a **mixture of continuous variables** (e.g. fitness) **and categorical variables** (e.g. GO terms)
- features contain **multiple labels per data unit/gene** (e.g. GO terms)
- contains **different coverage** of labels per gene (unbalanced classification)
- categorical input data is of **varying quality**, annotations can be wrong
- continuous input data **(fitness) is noisy**, quantification can be biased by experimental setup (sampling time and strategy, sgRNA binding efficiency, sequencing noise)

## Appropriate ML algorithms for the problem

According to different reviews dealing with ML application to biological problems ([Greener et al., Nat Rev Mol Cell Bio, 2021](https://www.nature.com/articles/s41580-021-00407-0) and [Mahood et al., Appl Plant Science, 2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7394712/)), some types of models/frameworks are more appropriate for this type of data than others. We have a **classification** and not a regression problem, which makes the following ML approaches most suitable:

1. Gradient boosting machine
2. Random forest
3. Multi-layer perceptron (neural network)
4. Support vector machine


# Data import and preprocessing

Load required packages.

```{r, message = FALSE }
suppressPackageStartupMessages({
  library(doParallel)
  library(tidyverse)
  library(ggrepel)
  library(ggpubr)
  library(caret)
  library(httr)
  library(scales)
})
```

- Load raw data with calculated fitness contribution of all genes and all tested conditions
- Load also KEGG pathways that were downloaded in the preceding pipeline

```{r}
df_gene <- read_csv("../data/output/fitness_genes.csv", col_types = cols())
df_kegg <- read_csv("../data/output/kegg_annotation.csv", col_types = cols())
```

- Load protein quantification (by mass spectrometry) from light and CO2 limitation experiments (from [Github](https://github.com/m-jahn/ShinyProt))
- Load protein quantification (by mass spectrometry) from perturbation experiments (from [Github](https://github.com/m-jahn/ShinyProt))

```{r}
#load(url("https://github.com/m-jahn/ShinyProt/blob/master/data/Jahn_2018_Light_and_CO2_lim.Rdata?raw=true"))
#load(url("https://github.com/m-jahn/ShinyProt/blob/master/data/Jahn_2018_Perturbations.Rdata?raw=true"))
```


```{r, echo = FALSE, warning = FALSE}
# custom ggplot2 theme that is reused for all later plots
custom_colors = c("#E7298A", "#66A61E", "#E6AB02", "#7570B3", "#666666", "#1B9E77", "#D95F02", "#A6761D")
custom_theme <- function(base_size = 12, base_line_size = 1.0, base_rect_size = 1.0, ...) {
  theme_light(base_size = base_size, base_line_size = base_line_size, base_rect_size = base_rect_size) + theme(
    title = element_text(colour = grey(0.4), size = 10),
    plot.margin = unit(c(12,12,12,12), "points"),
    axis.ticks.length = unit(0.2, "cm"),
    axis.ticks = element_line(colour = grey(0.4), linetype = "solid", lineend = "round"),
    axis.text.x = element_text(colour = grey(0.4), size = 10),
    axis.text.y = element_text(colour = grey(0.4), size = 10),
    panel.grid.major = element_line(size = 0.6, linetype = "solid", colour = grey(0.9)),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(linetype = "solid", colour = grey(0.4), fill = NA, size = 1.0),
    panel.background = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(colour = grey(0.4), size = 10, margin = unit(rep(3,4), "points")),
    legend.text = element_text(colour = grey(0.4), size = 10),
    legend.title = element_blank(),
    legend.background = element_blank(),
    ...
  )
}

# define custom set of graphical parameters for lattice
trellis.par.set(caretTheme())
```

## Mine data bases

Every protein is annotated by a wealth of different features that can be used to categorize proteins further. We can download a selection of the most interesting functional, biochemical, and phylogenetic annotations from [Uniprot](https://www.uniprot.org). Uniprot also contains functional annotation (terms) from other databases such as Pfam for structural motifs, KEGG for enzymes and pathways, or CHEBI for reaction chemistry. For an efficient download, we use their REST API. The list of available columns/database entries is available [here](https://www.uniprot.org/help/uniprotkb_column_names).

```{r}
# List of Uniprot features
uniprot_features <- c(
  "database(KEGG)",
  "id",
  "genes",
  "genes(PREFERRED)",
  "protein_names",
  "length",
  "mass",
  "ec",
  "chebi-id",
  "rhea-id",
  "go-id",
  "database(Pfam)",
  "feature(ACTIVE_SITE)",
  "feature(BINDING_SITE)",
  "feature(DNA_BINDING)",
  "comment(SUBUNIT)",
  "comment(SUBCELLULAR_LOCATION)",
  "feature(DISULFIDE_BOND)",
  "feature(MODIFIED_RESIDUE)",
  "feature(SIGNAL)",
  "feature(COILED_COIL)",
  "feature(REPEAT)",
  "feature(ZINC_FINGER)"
)


uniprot_url <- paste0(
  "https://www.uniprot.org/uniprot/?query=taxonomy:1111708&format=tab&columns=",
  paste(uniprot_features, collapse = ",")
)

get_uniprot <- function(url) {
  # reset security level, caused by a faulty SSL certificate on server side,
  # see this thread: https://github.com/Ensembl/ensembl-rest/issues/427
  httr_config <- config(ssl_cipher_list = "DEFAULT@SECLEVEL=1")
  res <- with_config(config = httr_config, GET(url))
  server_error = simpleError("")
  df_uniprot <- tryCatch(
    read_tsv(content(res), col_types = cols()),
    error = function(server_error) {
      message("Uniprot server not available, check internet connection")
    }
  )
}

df_uniprot <- get_uniprot(uniprot_url)
```

Many of the columns come in a long, list-like string format, which is not ideal to work with. These columns have to be reduced to lists of IDs or factors with fewer levels in order to be usable as features. For example, a list-like string of binding sites like this is not useful:

```
BINDING 56;  /note="Glycine";  /evidence="ECO:0000250"; BINDING 169;  /note="FAD; 
via amide nitrogen and carbonyl oxygen";  /evidence="ECO:0000250"; BINDING 298;  
/note="Glycine";  /evidence="ECO:0000250"; BINDING 324;  /note="Glycine";  /evidence=
"ECO:0000250"; BINDING 559;  /note="DXP; via amide nitrogen";  /evidence="ECO:0000250"
```

It can be reduced to either the position `56, 169, 298, 324, 559` or to the binding molecules `glycine, fad, dxp`.

```{r}
df_uniprot <- df_uniprot %>%
  
  # reformat col names
  rename_with(tolower) %>%
  rename(locus = `cross-reference (kegg)`) %>%
  rename(pfam = `cross-reference (pfam)`) %>%
  rename_with( ~ str_replace_all(., " \\[cc\\]", "")) %>%
  rename_with( ~ str_replace_all(., " ", "_")) %>%
  
  # reformat locus tag
  separate_rows(locus, sep = ";syn:") %>%
  mutate(locus = str_remove_all(locus, "syn:|;")) %>%
  filter(!is.na(locus)) %>%
  
  # reformat annotation cols
  # drop alternative EC numbers, they are sparse
  mutate(ec_number = str_extract(ec_number, "[0-9\\-]+\\.[0-9\\-]+\\.[0-9\\-]+\\.[0-9\\-]+")) %>%
  mutate(
    ec_level_1 = str_extract(ec_number, "[0-9\\-]+"),
    ec_level_2 = str_extract(ec_number, "[0-9\\-]+\\.[0-9\\-]+"),
    ec_level_3 = str_extract(ec_number, "[0-9\\-]+\\.[0-9\\-]+\\.[0-9\\-]+")
  ) %>%
  
  # condense string like annotation cols to simple factor or logical
  mutate(active_site = str_count(active_site, "ACT_SITE") %>% replace_na(0)) %>%
  mutate(binding_site = str_count(binding_site, "BINDING") %>% replace_na(0)) %>%
  mutate(dna_binding = str_count(dna_binding, "DNA_BIND") %>% replace_na(0)) %>%
  mutate(subunit_structure = str_extract(subunit_structure, "[a-zA-Z]+mer") %>%
    tolower) %>%
  mutate(subcellular_location = str_remove_all(
    subcellular_location, "SUBCELLULAR LOCATION: | \\{.*|[\\.;].*") %>%
    tolower) %>%
  mutate(across(matches("disulfide|modified|signal|coiled|repeat|zinc"),
    ~ as.numeric(!is.na(.))))
```

The next step is to spread list-like columns to wide format. For example, `gene_ontology_ids` contains a concatenated list of GO term IDs, with differing number of terms for each gene, and often very little overlap between genes. To be able to do this efficiently for different list-like columns (GO terms, CHEBI IDs, Pfam IDs, ...), a function is created that can do this automatically for each selected column. The function has a minimum requirement of e.g. at least 5 genes/observations annotated with a (GO) term. This was done in order to reduce the number of sparse mostly-zeroes columns.

```{r}
spread_list_col <- function(
  df, target_col, id_col, sep = "; ", n_min = 5, n_max = 100
) {
  select(df, all_of(c(target_col, id_col))) %>%
  mutate(dummy = 1) %>%
  separate_rows({{target_col}}, sep = sep) %>%
  group_by(across(all_of(target_col))) %>%
  filter(between(n(), n_min, n_max)) %>%
  pivot_wider(id_cols = {{id_col}}, names_from = {{target_col}},
    values_from = dummy, values_fill = 0) %>%
  right_join(df, by = {{id_col}}) %>%
  select(-{{target_col}})
}

# spread GO terms
df_uniprot_train <- spread_list_col(df_uniprot, "chebi_ids",
  "locus", n_min = 10, n_max = 200)

# select final list of features for classification
df_uniprot_train <- select(df_uniprot_train,
  !c(entry, gene_names, `gene_names__(primary_)`, gene_ontology_ids, #chebi_ids,
  protein_names, ec_number, ec_level_2, ec_level_3, rhea_id, pfam))
```


## Inspect data properties

According to objective 1, we can try to classify genes into different KEGG pathways. Many genes do not fall into one of these categories, because they are not enzymes, i.e. do not have an enzymatic activity in the strict sense of a metabolic pathway, but have a structural or regulatory function. Let's first get an overview about the annotation of the data set.

How many genes do have KEGG annotation? How many pathways are annotated per gene?

```{r, fig.width = 6, fig.height = 3.5}
paste("total number of genes:", unique(df_gene$locus) %>% length)
paste("total number of gene-KEGG pathway annotations:", nrow(df_kegg))
paste("number of genes annotated with KEGG pathway:",
  inner_join(filter(df_kegg, !duplicated(locus)),
    filter(df_gene, !duplicated(locus)), by = "locus") %>% nrow)

df_gene %>% select(locus) %>% distinct %>%
  left_join(df_kegg, by = "locus") %>%
  group_by(locus) %>%
  summarize(pw_per_gene = sum(!is.na(kegg_pathway))) %>%
  count(pw_per_gene) %>%
  
  ggplot(aes(x = pw_per_gene, y = n, label = n)) +
    geom_col() + labs(title = "KEGG annoations per gene") +
    geom_text(nudge_y = 80, color = grey(0.4), size = 3) +
    custom_theme()
```

How many genes are annotated per pathway? We can see that many associations are related to very general, hence non-informative "pathways" such as `Metabolic pathways`.

```{r, fig.width = 6, fig.height = 3.5}
df_gene %>% select(locus) %>% distinct %>%
  inner_join(df_kegg, by = "locus") %>%
  group_by(kegg_pathway) %>% count(sort = TRUE) %>% ungroup %>%
  mutate(rank = 1:n()) %>%
  mutate(kegg_pathway = if_else(rank <= 10, kegg_pathway, "")) %>%

  ggplot(aes(x = rank, y = n, label = kegg_pathway)) +
    geom_point(color = grey(0.4)) + labs(title = "Genes per KEGG pathway") +
    geom_text_repel(size = 3, color = grey(0.6), max.overlaps = 50) +
    custom_theme()
```

## Preparing the response variable

The response variable is the list of classes that we want to predict. For the initial test, KEGG pathways are used. Not all pathways are equally informative, some contain too many members and some too few to be distinctive for a set of genes. Many genes are annotated with more than one class, making the prediction even more difficult as we have overlapping predictor sets for different responses. For a start, the most generic pathways are removed. Then we can also identify largely overlapping classes by similarity clustering.

```{r, fig.width = 10, fig.height = 6}
df_kegg <- df_kegg %>% 
  group_by(kegg_pathway_id) %>%
  filter(between(n(), 3, 100)) %>% ungroup

hclust_kegg <- df_kegg %>%
  select(kegg_pathway, locus) %>%
  mutate(dummy = 1) %>%
  pivot_wider(kegg_pathway, names_from = locus,
    values_from = dummy, values_fill = 0) %>%
  column_to_rownames("kegg_pathway") %>% as.matrix %>%
  dist(method = "binary") %>% hclust(method = "complete")

# plot dendrogramm showing group similarity
plot(hclust_kegg, cex = 0.6, main = NA)
```

Extract labels that are highly similar to each other, that means that have many observations (genes) in common. We can do this by hierarchical clustering of labels (pathways) using a binary distance metric. For example, a distance of two labels of 0.5 means that these groups have 50% of their genes in common. Labels with strong overlap are removed.

```{r}
list_kegg_overlap <- cutree(hclust_kegg, h = 0.5) %>%
  duplicated %>% which %>% hclust_kegg$labels[.]
df_kegg <- filter(df_kegg, !kegg_pathway %in% list_kegg_overlap)
paste("removed", length(list_kegg_overlap), "highly similar pathways: ",
  paste(list_kegg_overlap, collapse = ", "))

# check genes with more than one pathway (label) annotated
df_kegg %>% group_by(locus) %>%
  summarize(pw_per_gene = sum(!is.na(kegg_pathway))) %>%
  count(pw_per_gene)

# number of unique pathways (labels)
df_kegg %>% pull(kegg_pathway_id) %>% unique %>% length

# number of unique genes/observations
df_kegg$locus %>% unique %>% length
```


## Transforming input data

In order to add **proteomics data** to the fitness measurements, the data needs to be reshaped to wide format, log-transformed, and re-scaled to obtain a symmetric range. The re-scaling requires more attention as compared to simple unrelated variables, because the log2 FC of all conditions should be comparable (that means centered around zero). Proteomics data is currently not included as it was found to not improve predictive power.

```{r, eval = FALSE, include = FALSE}
# prepare proteomics data
trim_to_range <- function(x, low, high) {
  x <- replace(x, x < low, low)
  x <- replace(x, x > high, high)
  x
}
  
df_proteome <- bind_rows(
    rename(Jahn_2018_Light_and_CO2_lim, locus = gene),
    rename(Jahn_2018_Perturbations, locus = protein)) %>%
  filter(
    !condition %in% c("CO2-1-0", "Light-1000", "300-NT", "60-NT"),
    !locus %in% c("aph", "bla", "eYFP", "mobA", "repA")) %>%
  mutate(log2FC = log2(rel_intensity) %>% replace_na(0)) %>%
  group_by(locus) %>%
  mutate(mean_mass_fraction = log10(mean(mean_massfraction))) %>% ungroup %>%
  select(locus, condition, mean_mass_fraction, log2FC) %>%
  pivot_wider(names_from = condition, values_from = log2FC, values_fill = 0) %>%
  mutate(mean_mass_fraction = rescale(mean_mass_fraction)) %>%
  mutate(across(matches("CO2|Light|DCMU|GLC|EYFP"), 
    ~ trim_to_range(., -3, 3) %>% rescale(from = c(-3, 3), to = c(0,1))))
```

Factors or character values are non-informative for the ML models. Regardless of the algorithm, it is highly advisable to transform all character variables to numeric type (in fact, `logical`). The `dummyVars` function re-arranges the data to a full-factorial matrix, that means all combinations of factor and levels are spelled out explicitly in separate columns, encoded by `0` and `1`. The binary encoding is favorable over a simple conversion of factor to numeric, which might indicate a continuous relationship between levels that does not exist (e.g. think about `13` and `14` closer related to each other than `1` and `14`). This is also called **One Hot Encoding**.

First we generate a test data set containing the *response variable* `kegg_pathway`, and the *predictors*, for example weighted mean fitness (`wmean_fitness`) measurements in all conditions. Additional predictors can be added later. The genes that don't map to any KEGG pathway are removed as they have no value in the training/validation set. They can be used to make predictions and hence are assigned to the **prediction data set**.

```{r}
# prepare data for training
df_all <- df_gene %>%
  filter(time == 0, !is.na(locus)) %>%
  select(locus, condition, wmean_fitness) %>%
  # spread condition-fitness combinations to wide format
  pivot_wider(names_from = condition, values_from = wmean_fitness) %>%
  
  # optionally add proteomics measurements
  #left_join(df_proteome, by = "locus", fill = 0) %>%
  
  # merge with uniprot feature table
  left_join(df_uniprot_train, by = "locus") %>%
  
  # add response variable
  left_join(select(df_kegg, locus, kegg_pathway_id), by = "locus")
```

Convert factors to dummy variables. The formula interface is similar to R's standard `response variable ~ predictor variables`, predictors can be separated by `+`, or all variables are used (`.`). If the data supplied to `dummy_vars` is the original data.frame, the predictions are identical to the input data. Otherwise new data can be used as predictor. The response variable should be in the first column, and all other columns should contain predictors.

```{r}
dummy_vars <- df_all %>%
  select(kegg_pathway_id, matches("."), -locus) %>%
  dummyVars(kegg_pathway_id ~ ., data = df_all)

df_all_dummy <- predict(object = dummy_vars, newdata = df_all) %>%
  as_tibble %>%
  mutate(across(everything(), ~ replace_na(., replace = 0)))

# split dataset in training and prediction set. For the latter,
# no classes are available
df_prediction <- df_all_dummy %>% filter(is.na(df_all$kegg_pathway_id))
df_training <- df_all_dummy %>% filter(!is.na(df_all$kegg_pathway_id))
classes <- filter(df_all, !is.na(kegg_pathway_id)) %>% pull(kegg_pathway_id)

dim(df_training); dim(df_prediction)
```

## Eliminate zero variance predictors

Variables with very low diversity are problematic (very few non-zero or non NA values) because they do not add predictive power to the model and bring some algorithms to crash. From the caret package vignette: 
"The concern is here that these predictors may become zero-variance predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These 'near-zero-variance' predictors may need to be identified and eliminated prior to modeling." We check the input data for such variables using `caret`s `nearZeroVar` function.

```{r}
# the columns with low variance
list_zerovar_cols <- nearZeroVar(df_training, freqCut = 99/1, uniqueCut = 1)
df_training <- df_training %>% select(-all_of(list_zerovar_cols))
paste("Removed the following invalid columns:", paste(list_zerovar_cols, collapse = ", "))
```

Another quality control step is to check which variables in the data are linearly correlated, and potentially remove them. This is done using `findLinearCombos`.

```{r}
list_invalid_cols <- df_training %>% findLinearCombos %>% {.$remove}
df_training <- df_training %>% select(-all_of(list_invalid_cols))
paste("Removed the following invalid columns:", paste(list_invalid_cols, collapse = ", "))
```

## Centering, scaling, range transformations

To give equal weights to different variables, values can be (optionally) centered, automatically re-scaled, or re-scaled to e.g. a range between 0 and 1.

```{r}
df_training <- df_training %>%
  preProcess(method = "range") %>%
  predict(df_training)
```

Note that all pre-processing can also be done with a single call to `preProcess` and `predict`, like this:

```{r, eval = FALSE, include = FALSE}
df_training <- preProcess(df_training,
  method = c("range", "nzv", "corr")) %>%
  predict(df_training)
```

## Split data into training and test set

The next step is to split the processed data set into different chunks for training and validation. Here it is important to get a balanced proportion of target values (classes), in this case `kegg_pathway`s. We use a typical size distribution of 80% training and 20% test data. The test data set is not touched by the model fitting procedure. It serves as independent benchmark when model training is concluded.

```{r}
set.seed(123)
train_index <- createDataPartition(classes, 1, p = 0.75)[[1]]

# split into training and test/validation set
df_train_tra <- df_training[train_index, ]
df_train_val <- df_training[-train_index, ]

# likewise split target variable into training and test set
classes_tra <- factor(classes[train_index])
classes_val <- factor(classes[-train_index])
```


# Model training and tuning

The caret package makes it easy to automate the model testing and training steps. The model training procedure tests different parameter sets, and for each parameter set a certain number of iterations. It re-samples the data the indicated `number` of times, and for each parameter set a defined number of `repeats` (does not apply for all models). These options are global for all trainings.

```{r}
train_options <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 2,
  p = 0.8
)
```

## Gradient boosting machine

The initial model that is tried is a gradient boosting machine (`gbm`). A gradient boosting machine is conceptually similar to a random forest. It is also based on decision trees, with the difference that random forests are an ensemble of decision trees based on bootstrap aggregation (random sub-sampling of trees), while gradient boosting relies on iterative refinement of sub-sampled trees (called 'boosting').

The tuning parameters can be specified by the user, otherwise default values are used. The argument `tuneGrid` can take a data frame with columns for each tuning parameter.

```{r}
# look up model's tuning parameters
modelLookup('gbm')

# redefine these parameters manually
tune_options <- expand.grid(interaction.depth = c(1,3,5), 
  n.trees = c(1:5, 10, 20, 30, 40, 50),
  shrinkage = 0.1,
  n.minobsinnode = 20)
```

To train the model, `caret`s `train()` function is used with the following standardized input:
  
  - `x`: for the default method, a data.frame or matrix where samples are in rows and features are in columns
  - `y`: a numeric or factor vector containing the outcome for each sample
  - `method`: string specifying which classification or regression model to use
  - `trControl`: list of training options defined by `trainControl` (optional)
  - `tuneGrid`: table of combinations with desired tuning options (optional)

We register a certain number of cores to run the model training in parallel.

```{r, warning = FALSE}
system.time({
  # start parallel processing
  cl <- makePSOCKcluster(detectCores())
  registerDoParallel(cl)
  # train model, supress messages
  invisible(capture.output(
    fit_gbm <- train(
      x = df_train_tra,
      y = classes_tra,
      method = "gbm",
      trControl = train_options,
      tuneGrid = tune_options
      )
    ))
  # stop parallel processing
  stopCluster(cl)
})
```

## Random forest

A second decision based approach that can be tested is a Random forest. As mentioned before it is conceptually similar to gradient boosting, only that the ensemble of trees is made by bootstrap aggregation ('bagging', all trees sampled independently).


```{r, warning = FALSE}
modelLookup('parRF')

tune_options <- expand.grid(
  mtry = c(3:10,20,50,100)
)

system.time({
  # start parallel processing
  cl <- makePSOCKcluster(detectCores())
  registerDoParallel(cl)
  # train model
  fit_raf <- train(
    x = df_train_tra,
    y = classes_tra,
    method = "parRF",
    trControl = train_options,
    tuneGrid = tune_options
  )
  # stop parallel processing
  stopCluster(cl)
})
```


## Multi-layer perceptron

A multi-layer perceptron is a (simple) neural network with a limited number of hidden layers. It is useful for regression and classification tasks that have a lower degree of complexity. For example, object recognition in images is done with more complex convolutional neural networks (CNNs). All neural netowrks rely on a fixed number of 'neurons' or nodes that take feature values as input, transform them by applying a function, and pass on the output value to the next layer of neurons. The general form for the transformation function of a single node is the following (with $x_i$ being a feature value, $w_i$ a weight factor, $b$ a learnable bias, and $\sigma$ an activation/cost function):

$y = \sigma \times (\sum(w_i \times x_i) + b)$

Training a neural network usually means to vary (optimize) the weight parameter $w$ for each node such that the cost (difference between expected output and obtained output) is minimized. Here we use the `mlp` function from package `RSNNS`, with default tuning parameters.

```{r, warning = FALSE, results = 'hide'}
# look up model's tuning parameters
modelLookup('mlp')

tune_options <- expand.grid(
  size = c(3:10,15,20,25,30,35,40,45,50)
)

system.time({
  # start parallel processing
  cl <- makePSOCKcluster(detectCores())
  registerDoParallel(cl)
  # train model
  fit_mlp <- train(
    x = df_train_tra,
    y = classes_tra,
    method = "mlp",
    trControl = train_options,
    tuneGrid = tune_options
  )
  # stop parallel processing
  stopCluster(cl)
})
```

## Support vector machine

Another type of ML algorithm is a support vector machine, that is quite different from tree-based or neural network based algorithms. A SVM is a supervised machine learning algorithm for classification and regression problems. The SVM algorithm tries to reduce the number of dimensions of the data and finds boundaries between different 'clusters' of data points. The shape of these boundaries is determined by the **kernel function**. It can be linear in the most simple case, or a radial, polynomial, or other type of (non-linear) function. In the simplest case, a classification problem with 2 classes, characterized by two different variables, the boundary between the observations is a line. For three feature variables it is a plane, for n variables it is an **n-dimensional hyperplane**. The boundary between classes is characterized by having the **maximum margin towards the support vectors**, that is, the set of points representing the edge/boundary of a point cluster belonging to a class.

```{r, warning = FALSE, results = 'hide'}
# look up model's tuning parameters
modelLookup('svmRadial')

tune_options <- expand.grid(
  sigma = c(0.01, 0.02, 0.05),
  C = seq(0.1,1, 0.1)
)

# train_options
system.time({
  # start parallel processing
  cl <- makePSOCKcluster(detectCores())
  registerDoParallel(cl)
  # train model
  fit_svm <- train(
    x = df_train_tra,
    y = classes_tra,
    method = "svmRadial",
    trControl = train_options,
    tuneGrid = tune_options
  )
  # stop parallel processing
  stopCluster(cl)
})
```

## Export current models

This optional step saves the current best fit models to disk in order to prevent re-fitting the same model for each change of the pipeline.

```{r, eval = FALSE}
save(list = c("fit_gbm", "fit_raf", "fit_mlp", "fit_svm"), file = "../data/output/ML_models.Rdata")
```

```{r, eval = FALSE, include = FALSE}
load(file = "../data/output/ML_models.Rdata")
```


# Compare performance of different models

## Evaluating the training process

After training, we can evaluate the training and fitting process. Calling the fitted model shows the final optimal parameters after fitting. We also plot the accuracy per tuning parameter set, and other metrics, see `?plot.train`.

```{r, fig.width = 7, fig.height = 6, warning = FALSE}
# Training summary
print(fit_gbm)

# Accuracy per tuning parameter
ggarrange(ncol = 2, nrow = 2, align = "hv",
  ggplot(fit_gbm) + custom_theme() +
    custom_theme(legend.position = c(0.85,0.2)),
  ggplot(fit_raf) + custom_theme(),
  ggplot(fit_mlp) + custom_theme(),
  ggplot(fit_svm) + custom_theme() +
    custom_theme(legend.position = c(0.85,0.2))
)
```


A metric that is very interesting is the contribution of each variable to the overall discriminatory power of the model. We can calculate the relative influence of each variable/feature for the final models and plot it. Decision-tree based models support the extraction of variable contribution. For MLP and SVM models, this metric is not readily available.

```{r, fig.width = 7, fig.height = 6, warning = FALSE}
# make df of variable contribution for each model
bind_rows(.id = "model",
    GBM = summary(fit_gbm, plot = FALSE) %>% as_tibble(rownames = NULL),
    RAF = varImp(fit_raf, scale = FALSE)[["importance"]] %>%
    as_tibble(rownames = "var") %>%
    rename(rel.inf = Overall) %>% 
    mutate(rel.inf = 100*rel.inf/sum(rel.inf))
  ) %>%
  
  # plot as barchart
  group_by(model) %>%
  arrange(desc(rel.inf)) %>% slice(1:30) %>%
  mutate(var = str_remove(var, "subcellular_location")) %>%
  ggplot(aes(x = rel.inf, y = reorder(var, rel.inf), fill = model)) +
  geom_col() +
  labs(x = "variable contribution (%)", y = "") +
  custom_theme() +
  scale_fill_manual(values = custom_colors[1:2])
```

## Predictive power

Evaluating the quality of the model in terms of its predictions. The **confusion matrix** shows the correlation of predicted classes and actual classes for the test dataset.

```{r, fig.width = 8, fig.height = 8, warning = FALSE}
make_confusion_matrix <- function(prediction, reference) {
  confusionMatrix(
    data = factor(prediction, unique(reference)),
    reference = factor(reference, unique(reference))
  ) %>% as.matrix %>% t %>% 
  # optionally divide by pathway gene_number for relative accuracy
  #apply(2, function(x) x/sum(x)) %>%
  levelplot(par.settings = caretTheme(),
    xlab = "reference", ylab = "prediction",
    scales = list(cex = 0.5, col = grey(0.5), x = list(rot = 90)),
    panel = function(x, y, z, ...) {
      panel.levelplot(x, y, z, ...)
      #panel.text(x, y, labels = round(z, 3), cex = 0.7,
      #  col = if_else(z > 0.6*max(z), grey(0.95), grey(0.5)))
      panel.xyplot(seq_along(unique(x)), seq_along(unique(y)),
        pch = 19, cex = 0.2, col = grey(0.7), ...)
    }
  )
}

classes_pred_gbm <- predict(fit_gbm, newdata = df_train_val)
classes_prob_gbm <- predict(fit_gbm, newdata = df_train_val, type = "prob")
make_confusion_matrix(classes_pred_gbm, classes_val)

classes_pred_raf <- predict(fit_raf, newdata = df_train_val)
classes_prob_raf <- predict(fit_raf, newdata = df_train_val, type = "prob")
make_confusion_matrix(classes_pred_raf, classes_val)

classes_pred_mlp <- predict(fit_mlp, newdata = df_train_val)
classes_prob_mlp <- predict(fit_mlp, newdata = df_train_val, type = "prob")
make_confusion_matrix(classes_pred_mlp, classes_val)

classes_pred_svm <- predict(fit_svm, newdata = df_train_val)
classes_prob_svm <- classes_prob_mlp %>%
  apply(2, function(x) rep(0, length(x))) %>% as.data.frame
make_confusion_matrix(classes_pred_svm, classes_val)
```

We can look at best predictions in more detail. This table contains all detailed statistics for the validation data set.
The validation data set may contain the same observation/gene multiple times with different (related) labels. The final result table is therefore trimmed down to contain each gene only once together with the best matching prediction.

```{r}
df_validation <- filter(df_all, !is.na(kegg_pathway_id)) %>%
  select(locus, kegg_pathway_id) %>%
  left_join(by = "kegg_pathway_id",
    select(df_kegg, kegg_pathway, kegg_pathway_id) %>% distinct) %>%
  slice(-train_index) %>%
  
  # add results
  mutate(
    GBM = classes_pred_gbm,
    RAF = classes_pred_raf,
    MLP = classes_pred_mlp,
    SVM = classes_pred_svm
  ) %>%
  
  pivot_longer(cols = c("GBM", "RAF", "MLP", "SVM"),
    names_to = "model", values_to = "prediction") %>%
  arrange(factor(model, c("GBM", "RAF", "MLP", "SVM"))) %>%
  mutate(probability = lapply(
    list(classes_prob_gbm, classes_prob_raf, classes_prob_mlp, classes_prob_svm),
    function(x) apply(x, 1, function(x) max(x, na.rm = TRUE))) %>% unlist) %>%
  mutate(correct = prediction == kegg_pathway_id) %>%
  
  # trim duplicated genes/observations
  group_by(model, locus) %>%
  arrange(desc(correct)) %>% slice(1) %>%
  ungroup %>% mutate(model = factor(model, c("GBM", "RAF", "MLP", "SVM")))
```

Now with all results for the validation set in one data frame, it is easy to make performance comparisons.

```{r}
df_validation %>% group_by(model) %>%
  summarize(total = n(), 
    correct = sum(correct),
    correct_percent = 100*correct/total)
```


This table summarizes the classes/labels where the model showed the best predictive performance. Among other metrics we determine:

- **sensitivity** - fraction of relevant instances that were retrieved (also called recall, TP/(TP+FN))
- **specificity** - fraction of non-relevant instances that were retrieved (TN/(TN+FP))
- **precision** - fraction of relevant instances among the retrieved instances (TP/(TP+FP))

```{r}
df_summary <- df_validation %>%
  group_by(model, kegg_pathway_id) %>%
  summarize(.groups = "drop_last",
    kegg_pathway = kegg_pathway[1],
    n_genes = n(),
    n_true_pos = sum(correct),
    n_false_neg = sum(!correct)
  ) %>%
  left_join(by = c("model", "kegg_pathway_id"),
    df_validation %>%
    group_by(model, prediction) %>%
    summarize(.groups = "drop_last",
      n_false_pos = sum(!correct)) %>% 
    rename(kegg_pathway_id = prediction)
  ) %>%
  mutate(
    n_true_neg = sum(n_genes)-(n_genes+n_false_pos),
    sensitivity = n_true_pos/n_genes,
    specificity = n_true_neg/(n_true_neg + n_false_pos),
    precision = n_true_pos/(n_true_pos + n_false_pos)
  ) %>%
  arrange(model, desc(sensitivity)) %>%
  filter(sensitivity > 0)

print(df_summary)
```

All predicted classes with at least one correct prediction.

```{r, fig.width = 8, fig.height = 5}
df_summary %>%
  ggplot(aes(x = n_true_pos, y = reorder(kegg_pathway, n_true_pos), fill = model)) +
    geom_col(aes(x = n_true_pos+n_false_neg, y = reorder(kegg_pathway, n_true_pos)),
      width = 1, fill = grey(0.8)) +
    geom_col(width = 1) +
    labs(title = "correct predictions per pathway (sensitivity)", x = "", y = "") +
    facet_grid( ~ model) +
    scale_fill_manual(values = custom_colors[1:4]) +
    custom_theme(legend.position = 0)
```

We can also look at the probability of all predicted genes to belong to the predicted label. We can plot the probability of a label versus the cumulative sum of correctly predicted labels. This could show a probability cutoff at which more correct than incorrect predictions are made. The second plot shows a histogram of the model-estimated probability to determine a class versus the actual turnout, % correct labels per bin.


```{r, fig.width = 6, fig.height = 7}
plot_prob_correct <- df_summary %>%
  pivot_longer(sensitivity:precision, names_to = "metric", values_to = "value") %>%
  mutate(kegg_pathway_id = factor(kegg_pathway_id, unique(kegg_pathway_id))) %>%
  ggplot(aes(x = as.numeric(kegg_pathway_id), y = value, fill = model)) +
    geom_col(width = 1) +
    labs(title = "performance metrics for classes with corrrect predictions",
      x = "Kegg pathways ranked by sensitivity", y = "") +
    facet_grid(factor(metric, unique(metric)) ~ model) +
    scale_fill_manual(values = custom_colors[1:4]) +
    custom_theme(legend.position = "")

plot_hist_correct <- df_validation %>%
  mutate(bin = cut_width(probability, 0.1, center = 0.05)) %>%
  group_by(model, bin) %>% summarize(.groups = "drop", percent_correct = sum(correct)/n()) %>%
  mutate(percent_correct = case_when(model == "SVM" ~ 0, TRUE ~ percent_correct)) %>%
  ggplot(aes(x = bin, y = percent_correct, fill = model)) +
    geom_col(width = 1) +
    labs(title = "% correct predictions per probability bin", x = "", y = "") +
    facet_grid( ~ model) +
    scale_fill_manual(values = custom_colors[1:4]) +
    custom_theme(legend.position = "bottom") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size = 8))

ggarrange(nrow = 2, ncol = 1, heights = c(0.57, 0.43),
  plot_prob_correct,
  plot_hist_correct
)
```


# Function prediction of unknown proteins

From the different ML algorithms that were tested, the best performing ones are selected to predict gene function (here: association with KEGG pathways). Since the models have different predictive power for different pathways, a majority vote of prediction is used.

```{r}
# preprocess prediction data the same way as training data
df_prediction <- df_prediction %>% select(all_of(colnames(df_train_val)))
df_prediction <- df_prediction %>% preProcess(method = "range") %>% predict(df_prediction)
stopifnot(all(colnames(df_prediction) == colnames(df_train_val)))

df_prediction_final <- df_all %>% filter(is.na(kegg_pathway_id)) %>% 
  select(locus) %>% mutate(
    kegg_pathway_id_GBM = predict(fit_gbm, newdata = df_prediction),
    kegg_pathway_id_RAF = predict(fit_raf, newdata = df_prediction),
    kegg_pathway_id_SVM = predict(fit_svm, newdata = df_prediction),
    kegg_pathway_id_GBM_prob = apply(predict(fit_gbm, newdata = df_prediction, type = "prob"), 1, max),
    kegg_pathway_id_RAF_prob = apply(predict(fit_raf, newdata = df_prediction, type = "prob"), 1, max)
  ) %>%
 
  # order by probability
  rowwise() %>% mutate(
    aggr_prob = mean(c_across(ends_with("prob"))),
    kegg_pathway_id_consensus = tail(names(sort(table(c_across(matches("(GBM|RAF|SVM)$"))))),1),
    kegg_pathway_id_consensus_n = max(table(c_across(matches("(GBM|RAF|SVM)$")))),
    kegg_pathway_consensus = select(df_kegg, kegg_pathway_id, kegg_pathway) %>%
      deframe %>% {.[kegg_pathway_id_consensus]}
  ) %>% ungroup %>%
  filter(kegg_pathway_id_consensus_n >= 2) %>%
  arrange(desc(kegg_pathway_id_consensus_n), desc(aggr_prob))

# add uniprot gene names to filter genes with known function
df_prediction_final <- df_prediction_final %>%
  left_join(by = "locus", 
    select(df_uniprot, locus, gene_names, `gene_names__(primary_)`,
      protein_names, subcellular_location))
```

We can finally inspect the predictions for the top genes by consensus and probability.


```{r, fig.width = 8, fig.height = 12}
plot_list_hits <- lapply(filter(df_summary, sensitivity > 0.5) %>%
  pull(kegg_pathway) %>% unique, function(pw) {
    df_prediction_final %>%
    mutate(pw_n = factor(kegg_pathway_id_consensus_n, 2:3)) %>%
    filter(kegg_pathway_consensus == pw) %>%
    arrange(desc(aggr_prob)) %>% slice(1:10) %>%
    ggplot(aes(x = aggr_prob, y = reorder(locus, aggr_prob),
      fill = pw_n, label = `gene_names__(primary_)`)) +
      geom_col(width = 0.8) +
      geom_text(nudge_x = 0.03, size = 3.5, color = grey(0.4), hjust = 0) +
      labs(title = pw, x = "probability", y = "") +
      scale_fill_manual(values = c(colorspace::lighten(custom_colors[2], 0.4),
        custom_colors[2]), drop = FALSE) +
      custom_theme(legend.position = "right") +
      coord_cartesian(xlim = c(0, 1))
  }
)

ggarrange(ncol = 2, nrow = 4, plotlist = plot_list_hits)
```

This is a table with function prediction for the top genes where no name or function is annotated in uniprot.

```{r}
df_prediction_final %>%
  #filter(str_detect(protein_names, "^([a-zA-Z0-9]{7}|Uncharacterized) protein$")) %>%
  filter(is.na(`gene_names__(primary_)`), aggr_prob > 0.5) %>%
  arrange(desc(aggr_prob)) %>%
  select(locus, aggr_prob, kegg_pathway_id_consensus,
    kegg_pathway_consensus, protein_names)
```




# Session Info

```{r}
sessionInfo()
```

