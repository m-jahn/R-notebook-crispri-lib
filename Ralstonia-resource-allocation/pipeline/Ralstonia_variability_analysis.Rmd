---
title: "Variability analysis and protein utilization for *R. eutropha*"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook: 
    theme: spacelab
    toc: yes
---


## Description

This R notebook is a bioinformatics pipeline to **perform variability analysis with a resource allocation model** for the chemolithoautotroph *Ralstonia eutropha* (a.k.a. *Cupriavidus necator*).


## Libraries

```{r, message = FALSE}
# loading libraries
library(lattice)
library(latticeExtra)
library(latticetools)
library(tidyverse)
library(stringi)
```

## Data import

Define the data source directories. Some of them are external in the sense of not included in the accompanying data folder of this R notebook. These are located in the accompanying github repository for the resource allocation model that was used here. The resource allocation model can be found at my fork of [Bacterial-RBA-models](https://github.com/m-jahn/Bacterial-RBA-models).

```{r, message = FALSE}
Reutropha_proteomics <- "~/Documents/SciLifeLab/Resources/R_projects/ShinyProt/data/Ralstonia_eutropha.Rdata"
model_reactions <- "~/Documents/SciLifeLab/Resources/Models/genome-scale-models/Ralstonia_eutropha/simulations/essentiality/model_reactions.csv"
simulation_dir <- "~/Documents/SciLifeLab/Resources/Models/Bacterial-RBA-models/Ralstonia-eutropha-H16/simulation/variability_analysis/"
source("read_rba_result.R")
```


Read simulation data.

```{r}
# read simulation results
df_flux <- read_rba_result(list.files(simulation_dir, pattern = "fluxes_.*.tsv$", full.names = TRUE))
df_prot <- read_rba_result(list.files(simulation_dir, pattern = "proteins_.*.tsv", full.names = TRUE))
df_macr <- read_rba_result(list.files(simulation_dir, pattern = "macroprocesses_.*.tsv", full.names = TRUE))
```


## Correlation between predicted and experimentally determined proteome

To compare the predicted and experimental proteome composition, we load the required proteomcis data. Proteomics data are mass spectrometry measurements with label-free quantification of peptides. Protein quantification was performed by summing upp all peptide intensities per annotated protein. The proteomic measurement unit, mass fraction, needs to be turned into the unit of RBA model predictions to perform sensible comparisons. RBA generally uses molecule number (or more specifically mmol) instead of mass in g. Therefore we convert protein measurements either to mmol gDCW<sup>-1</sup> h<sup>-1</sup>, or the relative fraction of those, mol fraction (mmol protein X/mmol total protein).

To allow a fair comparison between measured and predicted data, it is necessary to aggregate (e.g. sum up) all protein abundances allocated to one reaction. The reason is that the model will only predict **protein abundance of the first of a range of iso-enzymes** for a particular reaction, while in reality another iso-enzyme might be more abundant (carry the majority of flux). This would lead to lower correlation between measured and predicted protein concentrations.


**Step 1: load proteomics data**

```{r}
load(Reutropha_proteomics)

# pick a condition matching simulations
Ralstonia_eutropha <- Ralstonia_eutropha %>% filter(growthrate == 0.25) %>%
  
  # select only required columns
  select(condition, uniprot, locus_tag, protein, mol_fraction, COG_Process) %>%
  
  # rename conditions
  ungroup %>% mutate(condition = recode(condition,
    `FA 0.25` = "formate", `FRC 0.25` = "fructose", `SUC 0.25` = "succinate", `NLIM 0.25` = "ammonium"))
```


**Step 2: load gene reaction associations obtained from genome scale model**

```{r}
df_model_reactions <- read_csv(model_reactions, col_types = cols()) %>%
  
  # filter for reactions with gene associations
  select(reaction_id, reaction_name, genes) %>% separate_rows(genes, sep = ", ") %>%
  filter(!is.na(genes))
```


**Step 3: Select and rename conditions from RBA simulation**

```{r}
df_prot <- df_prot %>%
  mutate(condition = case_when(
    carbon_source == "succ" ~ "succinate",
    carbon_source == "for" ~ "formate",
    carbon_source == "fru" & nitrogen_conc == 10 ~ "fructose",
    nitrogen_conc < 1 ~ "ammonium",
  ))
```

**Step 4: Merge protein measurements and predictions into master table**


```{r, message = FALSE}
df_prot_comp <- df_model_reactions %>%
  # join with proteomics data
  left_join(Ralstonia_eutropha, by = c("genes" = "locus_tag")) %>%
  
  # join with simulation data
  left_join(df_prot, by = c("genes" = "key", "condition")) %>%
  
  # determine number of reactions per protein
  group_by(condition, genes) %>% mutate(n_reactions = length(length(reaction_id))) %>%
  
  # calculate mol fractions
  group_by(condition) %>% mutate(
    predicted_mol_fraction = value/(sum(value, na.rm = TRUE) * n_reactions),
    measured_mol_fraction = mol_fraction/(sum(mol_fraction, na.rm = TRUE) * n_reactions),
    COG_Process = if_else(is.na(COG_Process), "Other", COG_Process)
  ) %>%
  
  # summarize by summing up protein abundance per reaction (NA treated as zero)
  group_by(condition, reaction_id, reaction_name) %>% 
  summarize(
    predicted_mol_fraction = sum(predicted_mol_fraction, na.rm = TRUE),
    measured_mol_fraction = sum(measured_mol_fraction, na.rm = TRUE),
    ratio_mol_fraction = predicted_mol_fraction/measured_mol_fraction,
    COG_Process =  COG_Process %>% table %>% sort %>% {.[1]} %>% names
  ) %>%
  
  # trim COG Processes to most important ones
  mutate(COG_Process = replace(COG_Process, 
      !grepl("Amino|Nucleot|Coenz|Energy|Transla", COG_Process), "Other"
    )
  )  
```

Now we perform a test. We check if all mol fractions per condition sum to unity as we would expect.
And they do.

```{r, message = FALSE}
df_prot_comp %>% group_by(condition) %>% 
  summarize(sum(measured_mol_fraction), sum(predicted_mol_fraction))
```

Next, we plot the predicted versus actual protein abundance (mol fraction) summed up per reaction. The correlation between measured and predicted proteome is much higher when k<sub>app</sub> values are used that were determined using the RBApy estim package (R<sup>2</sup> > 0.5). These k<sub>app</sub> are best fits for the measured proteome and the estimated flux disitrubtion obtained from flux sampling. When k<sub>app</sub> values obtained from BRENDA database were used instead, the correlation was very low (R<sup>2</sup> < 0.1).


```{r, fig.width = 7, fig.height = 6}
plot_prot_comp <- df_prot_comp %>% 
  filter(!predicted_mol_fraction == 0, !measured_mol_fraction == 0) %>%
  
  xyplot(log10(predicted_mol_fraction) ~ log10(measured_mol_fraction) | condition, .,
    groups = COG_Process, par.settings = custom.colorblind(),
    #selected = !(.[['ratio_mol_fraction']] %>% between(0.01, 100)),
    #labels = .[['reaction_id']],
    main = "predicted vs measured protein allocation (summarized per enzyme)",
    xlab = expression("log"[10]*" measured mol fraction"), 
    ylab = expression("log"[10]*" predicted mol fraction"),
    as.table = TRUE, between = list(x = 1, y = 1),
    scales = list(alternating = FALSE),
    pch = 19, alpha = 0.6, cex = 0.7,
    ylim = c(-7, 0), xlim = c(-7, 0),
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.xyplot(x, y, ...)
      #panel.directlabel(x, y, draw_box = TRUE, box_line = TRUE, ...)
      panel.abline(a = 0, b = 1, col = grey(0.3), lwd = 2, lty = 2)
      panel.key(labels = factor(df_prot_comp$COG_Process) %>% levels %>%
        substr(1, 11) %>% paste0("..."), corner = c(0.97, 0.03), points = FALSE, cex = 0.6)
      rsquared = summary(lm(y ~ x))$r.squared
      panel.text(-6, -0.5, labels = paste0("R^2 = ", round(rsquared, 3)))
    }
  )

print(plot_prot_comp)
```


## k<sub>app</sub> correction for outliers

We can also export a table with overview about highly over-predicted or under-predicted enzyme abundance that can be used to correct k<sub>app</sub> values and thus improve protein predictions. It is not necessary to include this part if the predictions are sufficiently good.

```{r, include = FALSE, eval = FALSE}
kapp_extra <- df_prot_comp %>%
  
  # filter zero values
  filter(!predicted_mol_fraction == 0, !measured_mol_fraction == 0) %>%
  
  # summarize as mean ratio of predicted vs observed mol fraction
  group_by(reaction_id) %>%
  summarize(ratio_mol_fraction = mean(ratio_mol_fraction)) %>%
  
  # filter table to include only proteins with 10- or higher-fold
  # mis-prediction
  arrange(desc(ratio_mol_fraction)) %>%
  filter(ratio_mol_fraction >= 20 | ratio_mol_fraction <= 0.05) %>%
  mutate(reaction_id = paste0("R_", reaction_id, "_enzyme"))

# add only reactions to existing kapp_extra table that were not included before
#full_join(kapp_extra, read_csv("../../calibration/kapp_extra.csv")) %>%
#  write_csv("../../calibration/kapp_extra.csv")
```


## Variability analysis -- random sampling of k<sub>app</sub>

### Quantifying utilized reactions

To estimate the flexibility of simulated metabolism and the flux variability of reactions, we can perform RBA model simulations with random sampling of enzyme efficiency. These simulations are performed for growth under a limiting substrate just as described in the previous section. But instead of using the fitted (optimal) values of k<sub>app</sub> obtained from `RBA estim` (see [Bulovic et al., 2019](https://linkinghub.elsevier.com/retrieve/pii/S1096717619300710)), k<sub>app</sub> values were randomly sampled from a log normal distribution of k<sub>app</sub> values. This distribution is centered around a mean of 4 and an sd of 1.1 and was obtained from fitted (optimal) k<sub>app</sub> distribution. This procedure is identical with the one published in [O'Brien et al., 2016](https://doi.org/10.1371/journal.pcbi.1004998). 

The resource allocation model was used to perform 200 simulations with randomly sampled k<sub>app</sub> values for each of four conditions. The fluxes and protein abundances were predicted by the RBA model. Quantifying the average flux and the flux variation per reation can tell us then which reactions (and associated proteins) are more often used than others, or which ones are not used at all (i.e. not required under the simulated conditions). First flux simulation data is imported.

```{r}
# import flux simulation data obtained from random kapp sampling
sampling_dir <- paste0(simulation_dir, c("fructose/", "succinate/", "formate/", "ammonium/"))
      
df_random_flux <- bind_rows(.id = "substrate",
  read_rba_result(list.files(sampling_dir[1], pattern = "fluxes_.*.tsv$", full.names = TRUE)),
  read_rba_result(list.files(sampling_dir[2], pattern = "fluxes_.*.tsv$", full.names = TRUE)),
  read_rba_result(list.files(sampling_dir[3], pattern = "fluxes_.*.tsv$", full.names = TRUE)),
  read_rba_result(list.files(sampling_dir[4], pattern = "fluxes_.*.tsv$", full.names = TRUE))
  ) %>%
  # tidy substrate names and rename some columns
  mutate(substrate = recode(substrate, 
    `1` = "fructose", `2` = "succinate", `3` = "formate", `4` = "ammonium")) %>%
  rename(flux = value, reaction_id = key)
```


The first step is to perform a **saturation analysis**. Each iteration of the random sampling uses a certain set of reactions (and associated proteins), but this set will often be similar as essential reactions are used all the time, while other reactions might only be used with a favorable efficiency. As we don't know the real turnover number and saturation of each enzyme, this gives a more robust picture of enzyme utilization. After a number of simulations no new reactions are used, the set of utilized reactions becomes 'saturated'. This saturation is plotted as a function of iteration number. We can see number of unique reactions increased drastically in the beginning and only marginally at the end, when now or very few 'new' reactions are utilized.


```{r, message = FALSE}
# summarize variation per reaction
plot_randsamp_iter <- df_random_flux %>% 
  
  # saturation of random sampling: at which iteration does reaction first appear?
  group_by(substrate, reaction_id) %>% 
  summarize(first_appearance = iteration[1]) %>%
  group_by(substrate, first_appearance) %>%
  summarize(reaction_ids = length(reaction_id)) %>%
  mutate(reaction_ids = cumsum(reaction_ids)) %>%
  
  xyplot(reaction_ids ~ first_appearance, .,
    par.settings = custom.colorblind(), pch = 19,
    groups = substrate,
    xlab = "N simulations", ylab = "N used reactions",
    type = "l", lwd = 2.5, cex = 0.7,
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.superpose(x, y, ...)
      panel.key(..., points = FALSE, lines = TRUE, corner = c(0.95, 0.05))
    }, panel.groups = function(x, y, ...) {
      panel.xyplot(c(rep(x, each = 2), 200), sort(c(y, y[-1], rep(tail(y, 1), 2))), ...)
    }
  )

print(plot_randsamp_iter)
```

----------

We can also investigate how many reactions are maximally used under which condition, or taken together.
And also, which reactions are always used and which are used condition specific (core versus non-core proteome). The latter does not make much sense, most reactions that are always used are used under all the tested conditions (similar core proteome).

```{r, message = FALSE}
# all reactions
df_random_flux %>% ungroup %>% pull(reaction_id) %>%
  unique %>% length

# used reactions per condition
df_random_flux %>%
  group_by(substrate) %>%
  summarize(
    total_used_reactions = length(unique(reaction_id)),
    used_always = sum(table(reaction_id) >= 200),
    used_100x_or_more = sum(table(reaction_id) >= 100),
    used_50x_or_more = sum(table(reaction_id) >= 50)
  )

# which reactions make up the core and the substrate specific proteome?
df_random_flux %>%
  
  # filter spurious reactions with less than 100 appearances
  group_by(reaction_id, substrate) %>%
  summarize(n_simulations = length(iteration)) %>%
  filter(n_simulations >= 200) %>%
  
  summarize(
    n_substrates = length(unique(substrate)),
    substrates = sort(unique(substrate)) %>% paste(collapse = ", ")
  ) %>%
  
  # now summarize the reactions per appearance and plot
  group_by(substrates) %>%
  summarize(length(reaction_id))
```


### Quantifying flux variabilty

The next step is to quantify the variability of flux per reaction. Different statistical metrics for centrality and variation can be obtained and related to each other. Theoretically, more 'essential' reactions should have a lower variability while less essential reactions have a higher variability (i.e. are used only with favorable efficiency). More 'important' or central reactions on the other hand should show higher flux, but not necessarily higher or lower variability.

```{r, message = FALSE}
plot_randsamp_prot <- df_random_flux %>% 
  
  group_by(substrate, reaction_id) %>% summarize(
    median_flux = abs(median(flux)), 
    sd_flux = sd(flux), 
    min_flux = min(flux), 
    max_flux = max(flux),
    CV = abs(sd_flux/median_flux)
  ) %>%
  
  filter(!(is.na(sd_flux) | is.na(median_flux))) %>%
  mutate(sd_flux = log10(sd_flux), median_flux = log10(median_flux)) %>%
  
  xyplot(sd_flux ~ median_flux, .,
    groups = substrate, alpha = 0.7,
    par.settings = custom.colorblind(), pch = 19, cex = 0.7,
    xlab = "log10 median flux", ylab = "log10 SD flux",
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.xyplot(x, y, ...)
      panel.key(..., corner = c(0.95, 0.05))
    }
  )

print(plot_randsamp_prot)
```

## Overview on protein utilization

The utilization of proteins is a **binary choice for every protein**: it is either covered in any type of model simulation, or not. The situation gets a bit more complicated because the model solver would always choose the first of two annotated iso-enzymes per reaction (in case of an `OR` rule). In reality, several isoenzymes might catalyze the same reaction and both be expressed.

We therefore consider all proteins as utilized that are part of a reaction that carries flux in any simulation, either as essential subunit (`AND` rule) or as possible iso-enzyme (`OR` rule). All other enzymes are considered not utilized. Two more groups complete the picture, proteins that are part of molecular machinery (ribosomes, chaperones, DNA polymerase, RNA polymerase), or proteins that are not included by the model. Proteins that are part of molecular machines are obtained from the RBA model defintion. Non-modeled but experimentally quantified proteins are obtained from proteomics data.

```{r, message = FALSE}
# we collect utilized and non-utilized
utilized_enzymes <- df_model_reactions %>%
  filter(reaction_id %in% df_random_flux$reaction_id) %>%
  pull(genes) %>% unique

df_machinery <- bind_rows(
  read_tsv(paste0(simulation_dir, "../../data/replication.tsv")),
  read_tsv(paste0(simulation_dir, "../../data/transcription.tsv")),
  read_tsv(paste0(simulation_dir, "../../data/ribosome.tsv")),
  read_tsv(paste0(simulation_dir, "../../data/chaperones.tsv"))
)
utilized_machinery <- df_machinery %>% pull(Entry)

# new df with annotated utilization
df_utilization <- Ralstonia_eutropha %>% 
  mutate(
    utilization = case_when(
      locus_tag %in% utilized_enzymes ~ "utilized enzymes",
      uniprot %in% utilized_machinery ~ "utilized machinery",
      (locus_tag %in% df_model_reactions$genes) & !(locus_tag %in% utilized_enzymes) ~ "non-utilized enzymes",
      TRUE ~ "non-modeled proteins"
    )
  )

# get summary of proteins per group (same for all conditions)
df_utilization %>% group_by(utilization) %>%
  filter(condition == "fructose") %>%
  summarize(n_proteins = length(locus_tag))
```

Now that proteins were mapped to reactions and utilization was flagged for each detected protein, we can summarize the actual mol fraction occupied for each level of utilization.


```{r, message = FALSE}
plot_utilization_bar <- df_utilization %>%
  
  # determine mol fraction per group
  group_by(condition, utilization) %>% 
  summarize(mol_fraction = sum(mol_fraction, na.rm = TRUE)) %>%
  mutate(utilization = utilization %>% factor(., unique(.)[c(1,4,3,2)])) %>%
  
  xyplot(mol_fraction ~ factor(condition), .,
    groups = utilization, 
    stack = TRUE, cex = 0.65,
    par.settings = custom.colorblind(),
    xlab = "", ylab = "mol fraction", ylim = c(0, 1.5),
    scales = list(alternating = FALSE), 
    col = grey(c(0.4, 0.5, 0.65, 0.8)),
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.barchart(x, y, box.ratio = 0.9, horizontal = FALSE, ...)
      panel.key(..., corner = c(0.02, 0.98), points = FALSE)
      panel.text(x = as.numeric(x)+0.45, rep(c(0.2, 0.94, 0.7, 0.45), 5), labels = round(y, 2), cex = 0.65)
    }
  )

print(plot_utilization_bar)
```

## Top non-utilized and non-modeled proteins per condition

```{r}
plot_top_unutilized <- df_utilization %>%
  
  # select only non-utilized groups
  filter(utilization == "non-utilized enzymes") %>%
  
  # select only top 10 proteins per group and condition
  group_by(protein) %>% 
  mutate(mean_mol_fraction = mean(mol_fraction, na.rm = TRUE)) %>%
  ungroup %>% arrange(desc(mean_mol_fraction)) %>% slice(1:60) %>%
  
  xyplot(mol_fraction ~ protein %>% factor(., unique(.)), .,
    par.settings = custom.colorblind(), 
    groups = condition, cex = 0.7,
    xlab = "", ylab = "mol fraction", ylim = c(0, 0.035),
    main = "mol fraction of top 15 non-utilized enzymes", as.table = TRUE,
    scales = list(alternating = FALSE, x = list(rot = 35, cex = 0.6)), 
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.barchart(x, y, horizontal = FALSE, stack = TRUE,...)
      panel.key(..., corner = c(0.98, 0.98), points = FALSE)
    }
  )

print(plot_top_unutilized)
```


```{r}
plot_top_unmodeled <- df_utilization %>%
  
  # select only non-utilized groups
  filter(utilization == "non-modeled proteins") %>%
  
  # select only top 10 proteins per group and condition
  group_by(protein) %>% 
  mutate(mean_mol_fraction = mean(mol_fraction, na.rm = TRUE)) %>%
  ungroup %>% arrange(desc(mean_mol_fraction)) %>% slice(1:60) %>%
  
  xyplot(mol_fraction ~ protein %>% factor(., unique(.)), .,
    par.settings = custom.colorblind(), 
    groups = condition, cex = 0.7,
    xlab = "", ylab = "mol fraction", ylim = c(0, 0.06),
    main = "mol fraction of top 15 non-modeled proteins", as.table = TRUE,
    scales = list(alternating = FALSE, x = list(rot = 35, cex = 0.6)), 
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.barchart(x, y, horizontal = FALSE, stack = TRUE,...)
      panel.key(..., corner = c(0.98, 0.98), points = FALSE)
    }
  )

print(plot_top_unmodeled)
```


## Export figures and data

```{r}
png("../figures/figure_protein_utilization.png", width = 1500, height = 2200, res = 200)
print(plot_prot_comp, position = c(0, 0.5, 1, 1), more = TRUE)
print(plot_randsamp_iter, position = c(0, 0.29, 0.5, 0.53), more = TRUE)
print(plot_utilization_bar, position = c(0.5, 0.3, 1, 0.53), more = TRUE)
print(plot_top_unutilized, position = c(0, -0.015, 0.5, 0.3), more = TRUE)
print(plot_top_unmodeled, position = c(0.5, 0, 1, 0.3))
grid::grid.text(label = c("A", "B", "C", "D", "E"), 
  x = c(0.02, 0.02, 0.5, 0.02, 0.5), y = c(0.98, 0.5, 0.5, 0.3, 0.3), gp = grid::gpar(cex = 1.2))
dev.off()
```


