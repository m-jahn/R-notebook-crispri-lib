---
title: "Constraints for an *R. eutropha* resource allocation model"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook: 
    theme: spacelab
    toc: yes
---


## Description

This R notebook is a bioinformatics pipeline to **collect constraints for a genome scale, resource allocation model** in the chemolithoautotroph *Ralstonia eutropha* (a.k.a. *Cupriavidus necator*).

A resource allocation model can be coarse-grained (few symbolic reactions) or have genome scale detail (all known biochemical reactions and their associated genes). However, both types of models need to be constrained by a set of parameters to make realistic predictions. Depending on the model frame work, constraints can be equality constraints (example: turnover number of an enzyme E kcat<sub>E</sub> = 100 s<sup>-1</sup>), or inequality constraints (0 s<sup>-1</sup> <= kcat<sub>E</sub> <= 100 s<sup>-1</sup>). 

This notebook has the purpose to collect **constant and growth-rate dependent constraints** as they are used in [RBA models](https://sysbioinra.github.io/RBApy/). In RBApy, apparent enzyme efficiencies (k<sub>app</sub>), protein abundance, molecular machine abundance (protein/macromolecule complexes), and fluxes can be constrained. RBApy has the following possibilities for custom constraints.

- constants (example: `A = 0.1`)
- linear relationship, e.g. with growth rate µ (example: `B = 2 * µ + 0.1`)
- Michaelis-Menthen like kinetics for k<sub>app</sub> (example: `kapp = kcat * [S] / ([S] + Km)`)

Different types of data were collected to constrain at least two major determinants of a resource allocation model.

1. **apparent enzyme efficiency k<sub>app</sub>**: Organism-specific estimations for the maximum possible k<sub>app</sub> (= k<sub>cat</sub>) can be downloaded from enzyme data base BRENDA. In this case we use a precompiled file `max_KCAT.txt` from another resource allocation framework, [GECKO](https://github.com/SysBioChalmers/GECKO/tree/master/databases). The alternative to using pre-estimated k<sub>cat</sub> values is to determine the maximum k<sub>app</sub> from enzyme concnetrations, and flux estimation using e.g. FVA or flux sampling.
2. **protein abundance**: protein abundance was determined with mass spectrometry globally for *R. eutropha* using different growth rates and carbon sources. This data will be used to estimate and constrain enzyme abundance, and non-enzyme protein abundance.


## Libraries

```{r, message = FALSE}
# loading libraries
library(lattice)
library(latticeExtra)
library(latticetools)
library(tidyverse)
library(stringi)
```

```{r}
# define local input and output directories
# Note: these are not part of the github repository
RBA_dir <- "../../../../Models/Bacterial-RBA-models/Ralstonia-eutropha-H16/data/"
prot_dir <- "../../../ShinyProt/data/"
```

## k<sub>app</sub> estimation using BRENDA

### Import data from GECKO

The precompiled turnover numbers were downloaded from [GECKO](https://github.com/SysBioChalmers/GECKO/tree/master/databases) and apparently created Aug 16, 2018. The table contains all possible k<sub>kat</sub> values per enzyme class. It also contains a rudimentary phylogenetic classification (bacteria, archae, eukaryota, and so on) that can be used to strip matching k<sub>kat</sub> values with regular expressions.

```{r, message = FALSE, warning = FALSE}
# load kcat data
df_kcat <- read_tsv("../data/input/GECKO_20200505_max_KCAT.tsv", col_names = FALSE)[-5] %>%
  set_names(c("EC_number", "substrate", "organism", "kcat"))

# preview data
head(df_kcat)
```

### Overview of k<sub>kat</sub> values

The first step is to reformat the data to a more usable shape. Then, we extract species annotation using regular expressions, and plot an examplary subset of the data. The goal here is to test if the k<sub>kat</sub> values are normal-distributed, if there are outliers, and how common outliers are for specific species or EC numbers. These precautions are taken to make sure that the (maximum) k<sub>kat</sub> values we select are representative, and not extreme measurements that are unlikely to exist under unphysiological conditions (most values in BRENDA are inferred from *in vitro* measurements though).

```{r}
# split organism column in three
df_kcat <- df_kcat %>% separate(organism, c("species", "phylogeny", "org_id"), sep = "//") %>%
  
  # replace placeholder star with NA
  mutate_all(function(x) na_if(x, "*")) %>%
  
  # clip 'EC' away from EC number
  mutate(EC_number = gsub("^EC", "", EC_number))

head(df_kcat)
```

Now that the data is in a reasonable shape, k<sub>kat</sub> value distributions can be plotted broken down by EC number, substrate or organism.

```{r, fig.width = 9, fig.height = 7}
df_kcat <- df_kcat %>% 
  
  # determine number of values per reactions
  group_by(EC_number) %>%
  mutate(n_kcat_values = length(kcat))
  
# plot
histogram(~ log10(kcat) | EC_number, 
  filter(df_kcat, n_kcat_values >= 75),
  as.table = TRUE, between = list(x = 0.5, y = 0.5),
  par.settings = custom.colorblind(), border = "white",
  scales =list(alternating = FALSE), ylim = c(-5, 60),
  panel = function(x, ...){
    panel.grid(h = -1, v = -1, col = grey(0.9))
    panel.histogram(x, ...)
  }
)
```

It becomes quite clear that some of the k<sub>kat</sub> value distributions contain extreme outliers (1 to 2 orders of magnitude). Extreme values are biologically possible because the k<sub>kat</sub> can be different for different substrates or organism. Nevertheless it is wise to focus on the majority vote, and exclude k<sub>kat</sub> values that deviate too much. We filter the 5 % upper and lower quantile if more than 5 k<sub>kat</sub> values are available per reaction.

```{r, fig.width = 9, fig.height = 7}
df_kcat <- df_kcat %>%
  
  # filter the central 90 % of values (discard upper and lower 5 %)
  filter(!(n_kcat_values >= 5 & 
    (kcat < quantile(kcat, 0.05) | kcat > quantile(kcat, 0.95))
  ))

# plot
histogram(~ log10(kcat) | EC_number, 
  filter(df_kcat, n_kcat_values >= 75),
  as.table = TRUE, between = list(x = 0.5, y = 0.5),
  par.settings = custom.colorblind(), border = "white",
  scales =list(alternating = FALSE), ylim = c(-5, 60),
  panel = function(x, ...){
    panel.grid(h = -1, v = -1, col = grey(0.9))
    panel.histogram(x, ...)
  }
)
```

### Function to retrieve k<sub>kat</sub> values as model constraints

The last step is to devise a function that will match annotated EC numbers from the metabolic model with k<sub>kat</sub> values from the BRENDA database. It needs to perform the following steps sequentially. For each enzyme E, 

1. look up possible EC numbers in data base
2. if a *Ralstonia*/*Cupriavidus* entry is available, choose it
3. else look up *Burkholderiales* and take the median of the subset
4. else look up *betaproteobacteria* and take the median of the subset
5. else if none of these categories are available, take the median of all

The function is implemented in a versatile self-contained way so that it can be reused and adapated any time.
It takes the following input parameters:

- `kcat_data` - data frame with k<sub>kat</sub> values (imported directly from GECKO). Needs at least columns 'EC_number', 'species', 'phylogeny', and 'kcat'
- `ec_number` - the EC number to look up
- `species` - the name of the target species
- `phylogeny` - a set of search terms to look up in order of decreasing specificity
- `fun_aggregate` - the function to aggregate a selection of k<sub>kat</sub> values, like max, mean, median, and so on

```{r}
# main retrieval function
get_kcat <- function(
  ec_number, kcat_data,
  species = NULL, phylogeny = NULL,
  fun_aggregate = median) {
  
  # remove NA kcats from input and filter by EC number
  kcat_data <- kcat_data %>% ungroup %>%
    filter(!is.na(kcat), EC_number %in% ec_number)
  
  # return NA if EC number is not present
  if (nrow(kcat_data) == 0) {
    return(NA)
  }
  
  # filter by species
  index <- FALSE
  if (!is.null(species)) {
    index <- grepl(species, kcat_data[["species"]])
    if (any(index)) kcat_data <- filter(kcat_data, index)
  }
  
  # filter by phylogenetic terms
  if (!is.null(phylogeny) & !any(index)) {
    for (key in phylogeny) {
      index <- grepl(key, kcat_data[["phylogeny"]])
      if (any(index)) kcat_data <- filter(kcat_data, index); break
    }
  }
  
  # aggregate values
  kcat_data %>% pull(kcat) %>% fun_aggregate
}
```

Now let's test function using regular expressions!

```{r}
get_kcat(
  ec_number = "1.1.1.1",
  kcat_data = df_kcat,
  species = "[Cc]upriavidus|[Rr]alstonia",
  phylogeny = c("[Bb]urkholderiales", "[Bb]etaproteobacteria")
)
```

### Retrieve k<sub>kat</sub> values and export

The final step is to apply the function to each EC number that is present in the model. We retrieve only **full matches**, not partial matches like `1.2.3.-`. We retrieve the **minimum, median, and maximum k<sub>kat</sub> values** per reaction if several are available. Depending on the issue of over-constraining a resource allocation model, we can select the optimal/most realistic option. 

```{r, message = FALSE, warning = FALSE}
# load model reactions
df_model <- read_csv("../data/input/model_reactions.csv")[-1]
head(df_model)

# preprocessing EC numbers
df_model <- df_model %>%
  mutate(EC_number = EC_number %>% 
    str_replace_all("\\[|\\]|'", ""))

# retrieve kcat values and add new column to reaction data
for (fn in c("min", "median", "max")) {
  df_model[[paste0("kcat_", fn)]] <- sapply(
    df_model$EC_number, USE.NAMES = FALSE, function(ec_number) {
      ec_number %>% str_split(", ") %>% unlist %>%
      get_kcat(
        df_kcat,
        fun_aggregate = get(fn),
        species = "[Cc]upriavidus|[Rr]alstonia", 
        phylogeny = c("[Bb]urkholderiales", "[Bb]etaproteobacteria")
      )
    }
  )
}
```

Next we will have a look at the distribution of the retrieved minimal, maximal and median k<sub>kat</sub> values. We plot the distribution of k<sub>kat</sub> values on a log10 scale. The spread of k<sub>kat</sub> values is on average around 2 orders of magnitude between minimum and maximum (global medians around 1, 10, 100, see figure below). From the quantile analysis it seems that the central 75 % of k<sub>kat</sub> values lie between 2 and 53 with a global median of 11.1 1/s (median k<sub>kat</sub>s). This is extremely close to an average default k<sub>kat</sub> of 12.5 1/s suggested in Bulovic et al., 2019, but lower than the default values used in CobraME (65 1/s) and ETFL (172 1/s).

```{r, fig.width = 8}
# overview about kcat value distribution
histogram(~ kcat_min + kcat_median + kcat_max, 
  df_model,
  breaks = seq(-7, 7, 0.3), 
  par.settings = custom.colorblind(), border = "white",
  scales = list(alternating = FALSE, x = list(log = 10)),
  panel = function(x, ...){
    panel.grid(h = -1, v = -1, col = grey(0.9))
    panel.abline(v = -7:7, col = grey(0.9))
    med = median(x, na.rm = TRUE)
    panel.abline(v = med, col = grey(0.4), lty = 2, lwd = 1.5)
    panel.text(-4, 3, label = paste0("m = ", round(10^med, 3), " 1/s"), col = grey(0.4))
    panel.histogram(x, ...)
  }
)

# use quantiles on log 10 scaled kcats and scale back 
# (we want data normally distributed)
quantile(log10(df_model$kcat_median), na.rm = TRUE) %>% {10^.}
```

---------

The final step is to export the data frame in a format that is suitable for RBApy input. In this case the format requirements are:

- tab-separated values, no header
- units in `1/h` instead of `1/s` (x 3600)
- reaction ID in column 1, format (`R_`)`ID_enzyme` or `ID_transporter`
- max k<sub>kat</sub> in column 2
- min k<sub>kat</sub> in column 3 (backward efficiency, can be identical)
- no NA values (only complete rows)

```{r}
df_export <- df_model %>%
  select(reaction_id, kcat_median) %>% rename(kcat = kcat_median) %>%
  mutate(reaction_id = paste0("R_", reaction_id, "_enzyme")) %>%
  mutate(kcat = kcat * 3600, kcat_2 = kcat) %>%
  filter(!is.na(kcat))

write_tsv(df_export, "../data/output/enzyme_efficiency_brenda.tsv", col_names = FALSE)
```

----------

## k<sub>app</sub> estimation using proteomics/fluxomics

### Background

The second approach of estimating k<sub>app</sub> values is to perform an estimation based on in-house measurements instead of using published k<sub>cat</sub> values from literature or databases. The k<sub>app</sub> constants are actually a collective parameter, a helper construct to link enzyme fluxes to enzyme abundance, so that flux v = k<sub>app</sub> * [E]. The k<sub>app</sub> includes also the saturation s of the enzyme, so that k<sub>app</sub> ~ k<sub>cat</sub> * s. The saturation is between 0 and 1 so that k<sub>app</sub> is lower or equal to k<sub>cat</sub>.

However, the idea is that the k<sub>app</sub> parameter is kept constant for different conditions, while the flux (and the enzyme abundance) is allowed to change. We can then find different scenarios for enzyme saturation states:

- the flux through an enzyme and therefore its abundance increases in the model, **but not in measured enzyme abundance**: --> k<sub>app</sub> is higher than estimated, **enzyme not saturated**
- the flux through an enzyme and therefore its abundance increases in the model, **and in measured enzyme abundance**: --> k<sub>app</sub> is correctly estimated, **enzyme is saturated**

### Fluxomics and proteomics input

To estimate k<sub>app</sub> from available data, we only need to reformulate the simplified rate equation to:  k<sub>app</sub> =  v / [E]. For this purpose, we need two types of information:

1. **enzyme abundance in mmol gDCW<sup>-1</sup>** for known substrate uptakes rates (chemostat cultivations, maximum growth rate 0.25 h<sup>-1</sup>). The maximum tested growth rate was chosen to obtain realistic enzyme saturation. Protein abundance data was obtained by MS measurements. The relative protein *mass fraction* was determined by dividing MS intensity per protein by sum of all intensities. The *mass fraction* (g/g total protein) was then converted to protein concentration in *mmol/gDCW* by multiplying it with estimated protein concentration of 0.65 g protein/gDCW (done previously), and then dividing by molar mass of each protein (g/mol).
2. **flux per enzyme, in mmol h<sup>-1</sup> gDCW<sup>-1</sup>** for the corresponding substrate uptake rates under point 1. This information is obtained from flux sampling (FBA) simulations that were constrained to realistic flux distributions using data from [Alagesan *et al*., 2017](http://dx.doi.org/10.1007/s11306-017-1302-z). Input and output fluxes (for exchange reactions) were determined from chemostat cultivations.

----------

**Determine enzyme abundance for standard conditions.** Import Ralstonia mass fraction for the four tested conditions, fructose, succinate, formate, and ammonium (N-) limitation (with C-source fructose). Growth rate in all conditions was fixed to µ = 0.25 h<sup>-1</sup>, the maximum growth rate used in chemostat experiments.

```{r}
# import proteomics data
load(paste0(prot_dir, "Ralstonia_eutropha.Rdata"))

# simplify condition strings
Ralstonia_eutropha <- Ralstonia_eutropha %>% ungroup %>%
  mutate(condition = sapply(condition, function(x){
    unlist(stri_extract_all_words(x))[1]}
  ))

# make new data frame to merge fluxomics and proteomics data
df_flux_protein <- Ralstonia_eutropha %>%
  
  # select only required columns and filter for highest mu
  mutate(condition = recode(condition, `FA` = "formate", `NLIM` = "ammonium", `FRC` = "fructose", `SUC` = "succinate")) %>%
  select(condition, locus_tag, growthrate, mass_g_per_gDCW, MolWeight) %>%
  filter(growthrate == 0.25) %>% 
  
  # calculate protein concentration in mmol/gDCW.
  # MW in kDa must be converted to g/mol, and concentration to mmol
  mutate(
    conc_mmol_gDCW = mass_g_per_gDCW * 1000 / (MolWeight * 1000),
    conc_mmol_gDCW = replace_na(conc_mmol_gDCW, 0)
  )

head(df_flux_protein)

# export to csv
for (cond in unique(df_flux_protein$condition)) {
  filter(df_flux_protein, condition == cond) %>% 
    select(-condition) %>%
    write_csv(paste0("../data/output/protein_concentration_", cond, ".csv"))
}
```

----------

**Determine reaction fluxes for same condition.** Several approaches were tested to obtain minimal and maximal fluxes per enzyme. The best approach turned out to be flux sampling with the additional constraint of reaching at least 95% of the maximum growth rate found by FBA. Flux sampling with 100 iterations was performed using COBRApy. We can see that there is a threshold of standard deviation ~ 1 above which variation gets very high for some reactions. These are artificial cycles. The exception to this is N-limitation that can have higher flux variability due to the surmount carbon supply (fructose is not limiting). The N-limitation condition was therefore omitted for kapp determination.

```{r, message = FALSE, warning = FALSE, fig.width = 8, fig.height = 5.5}
# import flux sampling results for four conditions, max tested growth rate
df_sampling <- lapply(list.files("../data/input/", pattern = "^FSA", full.names = TRUE), 
  read_csv) %>% bind_rows(.id = "condition") %>%
  mutate(condition = recode(condition, `1` = "formate", `2` = "succinate", 
    `3` = "ammonium", `4` = "fructose")) %>%
  select(-2) %>% 
  gather(key = "reaction_id", value = "flux", -condition) %>%
  
  group_by(condition, reaction_id) %>% summarize(
    flux_mmol_gDCW_h = median(flux), 
    sd_flux = sd(flux), 
    min_flux = min(flux), 
    max_flux = max(flux),
    CV = abs(sd_flux/flux_mmol_gDCW_h)) %>%
  filter(!grepl("EX_", reaction_id)) %>%
  arrange(desc(sd_flux)) %>%
  
  # join with single optimal solution from loopless FBA.
  # Will help us to identify free running cycles in FSA
  left_join(
    lapply(list.files("../data/input/", pattern = "^FBA", full.names = TRUE),
    read_csv) %>% bind_rows(.id = "condition") %>%
    mutate(condition = recode(condition, `1` = "formate", `2` = "succinate", 
      `3` = "ammonium", `4` = "fructose")) %>%
    rename(reaction_id = X1, flux_mmol_gDCW_h_FBA = loopless)
  )

  
# plot results; 
plot_sampling <- lapply(list(A = c(-50, 550), B = c(-3, 28)), function(ylim) {
  xyplot(sd_flux ~ seq_along(sd_flux) | condition,
    filter(df_sampling, sd_flux != 0),
    groups = condition, xlab = NULL, ylim = ylim,
    scales = list(alternating = FALSE),
    par.settings = custom.colorblind(), layout = c(4, 1),
    panel = function(x, y, ...) {
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.abline(h = 1, lty = 2, col = grey(0.5))
      panel.xyplot(x, y, ...)
      panel.key(..., points = FALSE, corner = c(0.95,0.95))
    }
  )
})

print(plot_sampling[[1]], split = c(1,1,1,2), more = TRUE)
print(plot_sampling[[2]], split = c(1,2,1,2))
```


```{r, message = FALSE}
# process flux distribution, e.g. by removing extremely high fluxes
df_sampling2 <- df_sampling %>%
  
  # Filter out summary and outdated reactions
  filter(!reaction_id %in% c("Biomass", "Maintenance", "Protein", "Carbohydrate", 
    "Phospholipid", "	Cofactors_and_vitamins", "CBBCYC", "PYK1", "PYK2", "PYK3",
    "DHFR2", "DHFR3", "DHFR2p", "DHFR3p")) %>%
  
  # replace extreme fluxes with a min and max estimated from loopless FBA
  mutate(
    min_flux = if_else(sd_flux > 1, -abs(flux_mmol_gDCW_h_FBA), min_flux),
    max_flux = if_else(sd_flux > 1,  abs(flux_mmol_gDCW_h_FBA), max_flux)
  ) %>%
  
  # add an error margin to the sampled min and max fluxes, to help the solver
  # find a feasible solution
  mutate(
    min_flux = min_flux-(abs(min_flux)*1), 
    max_flux = max_flux+(abs(max_flux)*1)
  ) %>%
  
  # re-formatting of table
  arrange(desc(abs(max_flux))) %>%
  mutate(reaction_id = paste0("R_", reaction_id))

head(df_sampling2)

# export to csv
for (cond in unique(df_sampling2$condition)) {
  filter(df_sampling2, condition == cond) %>% ungroup %>%
    select(-condition) %>%
    write_csv(paste0("../data/output/model_flux_sampling_", cond, ".csv"))
}
```


### Determine k<sub>app</sub> values

The final step is to merge both datasets by computing the enzyme abundance allocated to each reaction. The estimated k<sub>app</sub> is then determined by dividing the flux through the enzyme abundance available for this reaction. This step was not performed in this R notebook but using the RBA built-in functions from the `RBApy estim` folder. Briefly, flux boundaries and protein concentration data that was exported serves as input for the script `kapp.py` ([link](https://github.com/m-jahn/Bacterial-RBA-models/tree/master/Ralstonia-eutropha-H16)). This script performs a series of FBA and RBA simulations constrained by these input data. It then gives an estimation of the k<sub>app</sub> for a particular condition.

----------

## Fraction of protein per compartment

The RBA model takes as another input parameter (or constraint) the fraction of protein per compartment. This constraint is important as it allows the cell to have only a limited amount of protein in cytoplasm or membrane compartments, for example. This constraint can be constant or it can be growth rate dependent e.g. by a linear relationship.

The first step is to prepare a **table with protein abundance and localization**. Protein abundance can be in any unit according to the RBApy manual, but it's best to use `mol fraction` instead of `mass fraction`, as all other RBApy functions also use `mmol`. The `mol fraction` is already available in the processed data set. The built-in `estim` functions don't seem to be well supported in python3 and raise errors. We therefore do the estimation manually by fitting linear models to the growth rate-protein abundance relationship. The idea is similar to the RBA `estim` function but less complicated. We focus on the standard condition as a **training data set** (fructose as carbon source, no NH4+ limitation).

```{r}
df_protein <- Ralstonia_eutropha %>% 
  
  # select only required columns and spread to long format
  select(condition, locus_tag, growthrate, Psortb_localization, mol_fraction) %>%
  set_names(c("condition", "protein", "growthrate", "location", "mol_fraction")) %>%
  
  # match localization names to model, simplify
  mutate(location = recode(location, Unknown = "Cytoplasm", Cytoplasmic = "Cytoplasm")) %>%
  mutate(location = replace(location, location != "Cytoplasm", "Cell_membrane"))

head(df_protein)
```

Now we can summarize the data by taking the sum of `mol fraction` over condition and localization. A simple approach to finding linear functions, where all proteins of all locations sum to one for a specific growth rate, would be to fit linear models for all compartments except one (e.g. cytoplasmic proteins, the biggest compartment). This one will then get a linear model fitted from the residual protein mass. We would expect the error for `Cytoplasm` to be negligibly small as it is the biggest compartment. However it turned out that the linear models fitted to both compartments perfectly sum to one already (see below). An estimation from residual protein fraction is therefore not necessary.

```{r, message = FALSE}
# First retrieve the proteins that are not part of the model, which is needed
# to  calculate non-enzymatic fraction later on
model_genes <- c(
  read_tsv(paste0(RBA_dir, "protein_summary.tsv"))$IDENTIFIER,
  read_tsv(paste0(RBA_dir, "ribosome.tsv"))[["Gene names"]],
  read_tsv(paste0(RBA_dir, "chaperones.tsv"))[["Gene names"]],
  read_tsv(paste0(RBA_dir, "transcription.tsv"))[["Gene names"]],
  read_tsv(paste0(RBA_dir, "replication.tsv"))[["Gene names"]]
)

# extract locus_tags only
model_genes <- model_genes %>% stri_extract_first_regex(
  pattern = "H16_[A-Z][0-9]{4}|PHG[0-9]{3}")

# add new NE_protein column
df_prot_per_comp <- df_protein %>%
  mutate(NE_protein = !protein %in% model_genes) %>%
  group_by(condition, location, growthrate)
```


```{r, fig.width = 9, fig.height = 5, message = FALSE}
# we can see a slight increase in cytoplasmic proteins
# and decrease in cell membrane proteins with rowth rate
xyplot(prot_per_compartment ~ growthrate | condition * location, 
  #summarize relative NE protein fraction per compartment
  df_prot_per_comp %>%
    summarize(prot_per_compartment = sum(mol_fraction, na.rm = TRUE)),
  groups = location, ylim = c(-0.15, 1.15),
  par.settings = custom.colorblind(),
  scales = list(alternating = FALSE),
  panel = function(x, y, ...) {
    panel.grid(h = -1, v = -1, col = grey(0.9))
    panel.superpose(x, y, ...)
  }, panel.groups = function(x, y, ...) {
    panel.xyplot(x, y, ...)
    panel.lmlineq(x, y, r.squared = TRUE,
      pos = 3, offset = 0.6, ...)
  }
)

# We use the consensus fit instead of a single isolated condition
# The two linear models will sum to one for any arbitrary growth rate
fit_cytoplasm <- df_prot_per_comp %>%
  filter(location == "Cytoplasm") %>%
  summarize(prot_per_compartment = sum(mol_fraction, na.rm = TRUE)) %>%
  lm(prot_per_compartment ~ growthrate, .) %>% 
  .$coefficients %>% round(4)

fit_cell_membrane = df_prot_per_comp %>%
  filter(location == "Cell_membrane") %>%
  summarize(prot_per_compartment = sum(mol_fraction, na.rm = TRUE)) %>%
  lm(prot_per_compartment ~ growthrate, .) %>% 
  .$coefficients %>% round(4)


# raise error if compartments don't sum to one
lin_model <- function(mu, slope, intercept) slope * mu + intercept
stopifnot(
  lin_model(0.1, fit_cytoplasm[[2]], fit_cytoplasm[[1]]) +
  lin_model(0.1, fit_cell_membrane[[2]], fit_cell_membrane[[1]]) == 1
)

# print coefficients
print(fit_cytoplasm)
print(fit_cell_membrane)
```

----------

## Fraction of non-enzymatic protein per compartment

The previous calculation determined how the total protein pool is distributed over compartments. In this section, we will go one level deeper and determine the fraction of **non-enzymatic protein** per compartment, analogously to the previous calculation. That means every compartment's protein pool is further subdivided into two sectors, enzymatic and non-anzymatic proteins, and these sectors can have --again-- a linear dependency on growth rate. According to the RBApy manual, the proteins that are **considered non-enzymatic are all proteins not acting as enzymes or molecular machines** in the model (such as ribosomes, chaperones, DNA polymerase and so on). Non-enzymatic (NE) proteins are therefore all proteins not captured by the RBA model at all. The fraction of NE proteins per compartment is a percentage of the proteins for that compartment only (each compartment sums to 1).

```{r, fig.width = 9, fig.height = 5, message = FALSE}
# determine NE protein per compartment
df_ne_prot_per_comp <- df_prot_per_comp %>% group_by(NE_protein, .add = TRUE) %>%
  summarize(prot_per_compartment = sum(mol_fraction, na.rm = TRUE)) %>%
  summarize(ne_prot_per_compartment = 
    prot_per_compartment[2]/sum(prot_per_compartment)
  )
  
# the fraction of NE protein per compartment decreases with growth rate
xyplot(ne_prot_per_compartment ~ growthrate | condition * location,
  df_ne_prot_per_comp,
  groups = location, ylim = c(-0.15, 1.15),
  par.settings = custom.colorblind(),
  scales = list(alternating = FALSE),
  panel = function(x, y, ...) {
    panel.grid(h = -1, v = -1, col = grey(0.9))
    panel.superpose(x, y, ...)
  }, panel.groups = function(x, y, ...) {
    panel.xyplot(x, y, ...)
    panel.lmlineq(x, y, r.squared = TRUE,
      pos = 3, offset = 0.6, ...)
  }
)

# These two models describe the relative fraction of NE 
# proteins *within* a compartment
# We use the consensus fit instead of a single isolated condition
fit_ne_cytoplasm <- df_ne_prot_per_comp %>%
  filter(location == "Cytoplasm") %>%
  lm(ne_prot_per_compartment ~ growthrate, .) %>% 
  .$coefficients %>% round(4)

fit_ne_cell_membrane = df_ne_prot_per_comp %>%
  filter(location == "Cell_membrane") %>%
  lm(ne_prot_per_compartment ~ growthrate, .) %>% 
  .$coefficients %>% round(4)

# print coefficients
print(fit_ne_cytoplasm)
print(fit_ne_cell_membrane)
```

----------

**Summary:**
Regardless of the cultivation condition or substrate limitation, it is possible to see growth rate dependent trends. Overall, the fraction of cytoplasmic proteins increases slightly with growth rate while membrane-associated proteins decrease. This makes sense as often expression of transporters is downregulated if bacteria don't need to scavenge substrates. We also see that the absolute majority of proteins is located in the cytoplasm (> 85% under all conditions).

The non-enzymatic fraction of proteins (proteins *not covered* by the model) decreases with growth rate from around 60% o 50% for cytoplasmic proteins, but is relatively constant for membrane associated ones. Most membrane-associated proteins (90% of mol fraction) are not covered by the model. The following figure summarizes all growth rate dependent trends.

```{r, fig.width = 9, fig.height = 2.6, message = FALSE}
df_prot_per_comp %>% group_by(NE_protein, .add = TRUE) %>%
  summarize(prot_per_compartment = sum(mol_fraction, na.rm = TRUE)) %>%
  ungroup %>% mutate(
    location = recode(location, Cytoplasm = "CP", Cell_membrane = "CM"),
    location = paste(location, ifelse(NE_protein, "NE", "E"), sep ="_")) %>%
  filter(condition == "FRC") %>%
  
  xyplot( ~ prot_per_compartment | growthrate * condition, .,
    groups = factor(location), scales = list(draw = FALSE),
    xlab = "mol fraction of proteome, per compartment",
    par.settings = custom.colorblind(),
    cex = 0.6, border = grey(0.3),
    panel = function(x, y, ...) {
      panel.piechart(x, diameter_inner = 0.1, 
        diameter_sector = 0.15, ...)
    }
  )
```

----------

## Combining estimated k<sub>app</sub> values to global consensus k<sub>app</sub>

The purpose of this section is to merge different k<sub>app</sub> estimations from different conditions to obtain a **consensus k<sub>app</sub> estimation**. The reasoning here is that k<sub>app</sub> is estimated from dividing the (sampled or measured) metabolic flux through an enzyme by its abundance, providing an estimate of enzyme efficiency. This efficiency depends also on the saturation of the enzyme. Estimation of k<sub>app</sub> was therefore performed for highest available growth rates/fluxes for four different conditions, in order to obtain enzyme efficiency at the highest saturation among the tested conditions.

Example: Formate dehydrogenase is not used under growth on fructose (no or low efficiency) while it is used under growth on formate (high efficiency). The maximum k<sub>app</sub> should therefore be taken from formate condition.

### Import k<sub>app</sub> estimation data obtained from `RBA estim`

```{r, message = FALSE}
# data directory
kapp_calib_dir <- "../data/input/"

# setting global parameter
max_saturation = 0.5

# load kapp estimation from different conditions
conditions <- c("ammonium", "formate", "fructose", "succinate")
kapp_files <- paste0(kapp_calib_dir, "kapp_estimate_", conditions, ".csv")

df_kapp <- lapply(kapp_files, function(df) {
  read_tsv(df, col_names = c("reaction_id", "kapp_forward", "kapp_reverse"))}) %>% 
  bind_rows(.id = "condition") %>%
  
  # change condition names
  mutate(condition = recode(condition, !!!(conditions %>% setNames(1:4)))) %>%
  
  # add a saturation factor S, increasing kapp by 1/S
  mutate_at(vars(contains("kapp_")), function(x) x/max_saturation) %>%
  
  # apply median normalization to account for different in silico growth 
  # rates in FBA simulation (different total flux)
  group_by(condition) %>% 
  mutate(kapp_forward = kapp_forward*(median(.[["kapp_forward"]])/median(kapp_forward))) %>%
  mutate(kapp_reverse = kapp_forward) %>%
  
  # sort decreasingly by average kapp
  group_by(reaction_id) %>% 
  mutate(kapp_mean = mean(kapp_forward)) %>%
  arrange(desc(kapp_mean))

head(df_kapp)
```


```{r, fig.height = 4, fig.width = 7}
# plot kapp value distribution per condition
plot_kapp_dist <- lapply(c("condition", "all_conditions"), function(cond) {
  xyplot(log10(kapp_forward) ~ as.numeric(factor(reaction_id, unique(reaction_id))) | get(cond), 
    df_kapp %>% mutate(all_conditions = "all conditions"), 
    groups = condition,
    xlab = "number of reactions", ylab = bquote("log"[10]*"  k"[app]),
    as.table = TRUE, between = list(x = 0.5, y = 0.5),
    par.settings = custom.colorblind(),
    scales =list(alternating = FALSE),
    panel = function(x, y, ...){
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.xyplot(x, y, ...)
      med = median(y)
      panel.abline(h = med, lty = 2, lwd = 2, col = grey(0.5))
      panel.text(grid::unit(175, "npc"), med+3, 
        labels = paste0("median k_app = ", round(10^med, 2)))
      if (cond == "all_conditions") {
        panel.key(..., corner = c(0.98, 0.95))
      }
    }
  )
})

print(plot_kapp_dist[[2]])
```


```{r, fig.height = 6, fig.width = 7}
# the average kapp seems to be a reliable estimation already, although there 
# is some variation between conditions. We can look at the top 20 reactions
plot_kapp_minmax <- lapply(list(c(1, 80), c(1053, 1133)), function(xrange) {
  xyplot(log10(kapp_forward) ~ reaction_id %>% gsub("R_|_enzyme", "", .) %>% 
      factor(., unique(.)),
    df_kapp %>% ungroup %>% slice(xrange[1]:xrange[2]), groups = condition,
    as.table = TRUE, between = list(x = 0.5, y = 0.5),
    xlab = "", ylab = bquote("log"[10]*"  k"[app]),
    par.settings = custom.colorblind(),
    scales =list(alternating = FALSE, x = list(rot = 35)),
    panel = function(x, y, ...){
      panel.grid(h = -1, v = -1, col = grey(0.9))
      panel.xyplot(x, y, ...)
      panel.key(..., corner = c(0.01, 0.03))
    }
  )
})

print(plot_kapp_minmax[[1]], split = c(1,1,1,2), more = TRUE)
print(plot_kapp_minmax[[2]], split = c(1,2,1,2))
```


### Manual adjustment of k<sub>app</sub> for selected reactions

We import a table with correction factors for k<sub>app</sub> values that have been estimated incorrectly. This set of k<sub>app</sub> values was determined by comparing the correlation between predicted and measured protein abundance. Proteins that are more than 10 fold away from measured abundance are candidates for manual correction. Other reactions that need correction are missing k<sub>app</sub> values for important transporters. These are also included in the following section.

```{r, message = FALSE}
# set of adjusted kapp values for outlier enzymes not covered in kapp estimation
kapp_median <- median(df_kapp$kapp_forward)
kapp_extra <- read_csv(paste0(kapp_calib_dir, "kapp_extra.csv"))

# print mean and sd of log-transformed kapp distribution
df_kapp$kapp_forward %>% log10 %>% mean
df_kapp$kapp_forward %>% log10 %>% sd

# kapp estimation for reactions where information is missing
kapp_missing <- kapp_extra %>%
  filter(!(reaction_id %in% df_kapp$reaction_id)) %>%
  mutate(
    kapp_forward = ratio_mol_fraction*kapp_median,
    kapp_reverse = kapp_forward) %>%
  select(-ratio_mol_fraction)

# kapp estimation for transporters
kapp_transport <- data.frame(stringsAsFactors = FALSE,
  reaction_id = c("R_H2Ot_enzyme", "R_CO2t_enzyme", "R_O2t_emzyme", "R_SO4t_enzyme",
    "R_PIt2r_enzyme", "R_Ktr_enzyme", "R_NAt3_15_enzyme", "R_FE2abc_enzyme",
    "R_MG2t_enzyme", "R_COBALTt5_enzyme", "R_FORt_enzyme", "R_NH4t_enzyme", 
    "R_FRUabc_enzyme", "R_HEXf_enzyme", "R_SUCCt2r_enzyme", "R_FRUpts2_enzyme", 
    "R_SUCCabc_enzyme", "R_SUCFUMt_enzyme"),
  kapp_forward = c(rep(1000000, 10), rep(100000, 4), 10000, rep(0, 3))
  ) %>% mutate(kapp_reverse = kapp_forward)
```

The final consensus kapp table is prepared by adding missing kapps for transport reactions and correcting several falsely estimated kapps. The table is exported as `*.csv` file.

```{r, message = FALSE}
# Select maximum as aggregation metric
df_kapp_export <- df_kapp %>% group_by(reaction_id) %>%
  summarize(
    kapp_forward = max(kapp_forward),
    kapp_reverse = max(kapp_reverse)
  ) %>%
  
  # add missing kapps
  filter(!(reaction_id %in% c("R_HEXf_enzyme","R_FRUabc_enzyme"))) %>%
  bind_rows(kapp_missing) %>%
  bind_rows(kapp_transport) %>%
  
  # update kapp estimation for some reactions where estimation was inaccurate
  left_join(filter(kapp_extra, reaction_id %in% df_kapp$reaction_id)) %>%
  mutate(ratio_mol_fraction = replace_na(ratio_mol_fraction, 1)) %>%
  mutate(
    kapp_forward = kapp_forward * ratio_mol_fraction,
    kapp_reverse = kapp_reverse * ratio_mol_fraction
  ) %>%
  select(-ratio_mol_fraction)

# export final result
write_tsv(df_kapp_export, "../data/output/kapp_consensus.csv", col_names = FALSE)
```

Finally we can create a summary figure for the process of k<sub>app</sub> estimation.

```{r, message = FALSE, fig.width = 5.5, fig.height = 8}

print(plot_kapp_dist[[2]], position = c(0, 0.73, 1, 1), more = TRUE)
print(plot_kapp_dist[[1]], position = c(0, 0.42, 1, 0.78), more = TRUE)
```


